{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt\n",
    "\n",
    "import sys; sys.path.insert(0, '../') \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import cdist\n",
    "import mne\n",
    "\n",
    "from invert.forward import get_info, create_forward_model\n",
    "from invert.util import pos_from_forward\n",
    "from invert.evaluate import eval_mean_localization_error\n",
    "\n",
    "pp = dict(surface='inflated', hemi='both', verbose=0, cortex='low_contrast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Good channels</th>\n",
       "        <td>32 EEG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Bad channels</th>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Source space</th>\n",
       "        <td>Surface with 324 vertices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Source orientation</th>\n",
       "        <td>Fixed</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Forward | MEG channels: 0 | EEG channels: 32 | Source space: Surface with 324 vertices | Source orientation: Fixed>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "sampling = \"ico2\"\n",
    "info = get_info(kind='biosemi32')\n",
    "fwd = create_forward_model(info=info, sampling=sampling)\n",
    "fwd[\"sol\"][\"data\"] /= np.linalg.norm(fwd[\"sol\"][\"data\"], axis=0) \n",
    "pos = pos_from_forward(fwd)\n",
    "leadfield = fwd[\"sol\"][\"data\"]\n",
    "n_chans, n_dipoles = leadfield.shape\n",
    "\n",
    "source_model = fwd['src']\n",
    "vertices = [source_model[0]['vertno'], source_model[1]['vertno']]\n",
    "adjacency = mne.spatial_src_adjacency(fwd[\"src\"], verbose=0)\n",
    "distance_matrix = cdist(pos, pos)\n",
    "max_dist = distance_matrix.max()\n",
    "\n",
    "fwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from invert.simulate import generator\n",
    "sim_params = dict(\n",
    "    use_cov=False,\n",
    "    return_mask=False,\n",
    "    batch_repetitions=1,\n",
    "    batch_size=1,\n",
    "    n_sources=2,\n",
    "    n_orders=0,\n",
    "    # snr_range=(1, 1),\n",
    "    snr_range=(1e20, 1e21),\n",
    "    amplitude_range=(1, 1),\n",
    "    n_timecourses=200,\n",
    "    n_timepoints=50,\n",
    "    scale_data=False,\n",
    "    add_forward_error=False,\n",
    "    forward_error=0.1,\n",
    "    # inter_source_correlation=(0, 0.99),\n",
    "    inter_source_correlation=0,\n",
    "    return_info=True,\n",
    "    diffusion_parameter=0.1,\n",
    "    # correlation_mode=\"cholesky\",\n",
    "    # noise_color_coeff=(0, 0.99),\n",
    "    correlation_mode=None,\n",
    "    noise_color_coeff=0,\n",
    "    \n",
    "    random_seed=None)\n",
    "\n",
    "sim_params = dict(\n",
    "    use_cov=False,\n",
    "    return_mask=False,\n",
    "    batch_repetitions=1,\n",
    "    batch_size=1,\n",
    "    n_sources=(1, 10),\n",
    "    n_orders=(0, 0),\n",
    "    snr_range=(0.2, 10),\n",
    "    amplitude_range=(0.1, 1),\n",
    "    n_timecourses=200,\n",
    "    n_timepoints=50,\n",
    "    scale_data=False,\n",
    "    add_forward_error=False,\n",
    "    forward_error=0.1,\n",
    "    inter_source_correlation=(0, 1),\n",
    "    return_info=True,\n",
    "    diffusion_parameter=0.1,\n",
    "    correlation_mode=\"cholesky\",\n",
    "    noise_color_coeff=(0, 0.99),\n",
    "    \n",
    "    random_seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ CNN1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">205,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">603</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ CNN1 (\u001b[38;5;33mConv2D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m32\u001b[0m)      │         \u001b[38;5;34m1,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │       \u001b[38;5;34m205,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m603\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">206,659</span> (807.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m206,659\u001b[0m (807.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">206,659</span> (807.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m206,659\u001b[0m (807.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "# Assuming we have a function to generate initial EEG data and true dipoles\n",
    "def generate_initial_data(gen):\n",
    "    # This function should return initial EEG data\n",
    "    # and the true dipole parameters that generated the data.\n",
    "\n",
    "    # Generate random dipole parameters\n",
    "    x, y, _ = gen.__next__()\n",
    "    x = np.swapaxes(x, 1, 2)\n",
    "    y = np.swapaxes(y, 1, 2)\n",
    "    true_indices = [np.where(yy[:, 0]!=0)[0] for yy in y]\n",
    "    return x, true_indices, y\n",
    "\n",
    "def outproject_from_data(data, leadfield, idc, lam=0.001):\n",
    "    L = leadfield[:, idc]\n",
    "    Y_est = L.T @ np.linalg.pinv(L @ L.T + np.identity(L.shape[0])*lam) @ data\n",
    "    # return data - L@Y_est\n",
    "    return L@Y_est - data\n",
    "\n",
    "def wrap_outproject_from_data(current_data, leadfield, estimated_dipole_idc, lam=0.001):\n",
    "    # Wrapper function to outproject dipoles from the data\n",
    "    n_samples = current_data.shape[0]\n",
    "    new_data = np.zeros_like(current_data)\n",
    "    for i in range(n_samples):\n",
    "        new_data[i] = outproject_from_data(current_data[i], leadfield, np.array(estimated_dipole_idc[i]), lam=lam)\n",
    "    return new_data\n",
    "\n",
    "def predict(model, current_covs):\n",
    "    # Predict source estimate\n",
    "\n",
    "    # Predict the sources using the model\n",
    "    estimated_sources = model.predict(current_covs)  # Model's prediction\n",
    "    return estimated_sources\n",
    "    \n",
    "    # return new_data, estimated_dipole_idc\n",
    "\n",
    "# Function to compute residuals or stopping condition\n",
    "def compute_residual(current_data, new_data):\n",
    "    # Placeholder function to compute residual to decide when to stop the iteration\n",
    "    return tf.norm(current_data - new_data)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def best_match_idx(pos, predicted_position):\n",
    "    return np.argmin(np.sum((pos - predicted_position)**2, axis=1))\n",
    "\n",
    "def custom_loss(pos):\n",
    "    def loss(y_true, y_pred):\n",
    "        # pos is a tensor of shape (n, 3)\n",
    "        # y_true is of shape (batch_size, n), where each row has several '1's indicating the true positions\n",
    "        # y_pred is of shape (batch_size, 3)\n",
    "\n",
    "        # Calculate the squared distances between every pred and every pos\n",
    "        # pos_expanded: [1, n, 3]\n",
    "        # y_pred_expanded: [batch_size, 1, 3]\n",
    "        pos_expanded = tf.expand_dims(pos, axis=0)\n",
    "        y_pred_expanded = tf.expand_dims(y_pred, axis=1)\n",
    "        squared_distances = tf.sqrt(tf.reduce_sum(tf.square(pos_expanded - y_pred_expanded), axis=2))  # [batch_size, n]\n",
    "\n",
    "        # Use y_true as a mask to select relevant distances\n",
    "        max_distance = tf.reduce_max(squared_distances) + 1\n",
    "        masked_distances = tf.where(y_true > 0, squared_distances, max_distance)\n",
    "\n",
    "        # Get the minimum distance for each example in the batch\n",
    "        min_distances = tf.reduce_min(masked_distances, axis=1)\n",
    "\n",
    "        # Return the mean of these minimum distances as the loss\n",
    "        return tf.reduce_mean(min_distances)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# def custom_loss(pos):\n",
    "#     # pos is a constant tensor of shape (n, 3) representing fixed locations in xyz for each of the n points\n",
    "    \n",
    "#     def loss(y_true, y_pred):\n",
    "#         # y_true: tensor of shape (batch_size, n), binary labels\n",
    "#         # y_pred: tensor of shape (batch_size, 3), predicted positions in xyz\n",
    "        \n",
    "#         # Expand y_pred to shape (batch_size, 1, 3) to compute distances\n",
    "#         y_pred_expanded = tf.expand_dims(y_pred, axis=1)\n",
    "        \n",
    "#         # Compute squared Euclidean distances from predicted positions to all points in 'pos'\n",
    "#         # Resulting shape: (batch_size, n)\n",
    "#         distances = tf.sqrt(tf.reduce_sum(tf.square(pos - y_pred_expanded), axis=-1))\n",
    "        \n",
    "#         # Filter distances using y_true, setting distances to non-selected positions to a large number\n",
    "#         max_distance = tf.reduce_max(distances) + 1\n",
    "#         filtered_distances = tf.where(y_true > 0, distances, max_distance)\n",
    "        \n",
    "#         # Compute the minimum distance for each sample in the batch\n",
    "#         min_distances = tf.reduce_min(filtered_distances, axis=1)\n",
    "        \n",
    "#         # Return the mean of these minimum distances as the loss\n",
    "#         return tf.reduce_mean(min_distances)\n",
    "    \n",
    "#     return loss\n",
    "\n",
    "# Define the neural network architecture\n",
    "input_shape = (n_chans, n_chans, 1)  # Specify the input shape based on your data\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(32, (1, n_chans), \n",
    "          activation=\"tanh\", padding=\"valid\",\n",
    "          input_shape=input_shape,\n",
    "          name='CNN1'),\n",
    "    layers.Flatten(),\n",
    "    # layers.Dense(200, activation='relu'),\n",
    "    layers.Dense(200, activation='relu'),\n",
    "    layers.Dense(3, activation='linear')\n",
    "    # layers.Dense(3, activation='softmax')\n",
    "    # layers.Dense(3, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss=custom_loss(tf.constant(pos/max_dist, dtype=tf.float32)))  # Specify the loss function and optimizer\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Specify the loss function and optimizer\n",
    "model.build()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x000001D740BFE050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "epoch 0 0.22 (36.5 mm)\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x000001D740BFE050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "epoch 1 0.20 (33.7 mm)\n",
      "epoch 2 0.19 (32.1 mm)\n",
      "epoch 3 0.19 (31.1 mm)\n",
      "epoch 4 0.18 (30.3 mm)\n",
      "epoch 5 0.18 (29.7 mm)\n",
      "epoch 6 0.18 (29.3 mm)\n",
      "epoch 7 0.17 (28.9 mm)\n",
      "epoch 8 0.17 (28.6 mm)\n",
      "epoch 9 0.17 (28.3 mm)\n",
      "epoch 10 0.17 (28.0 mm)\n",
      "epoch 11 0.17 (27.8 mm)\n",
      "epoch 12 0.17 (27.6 mm)\n",
      "epoch 13 0.16 (27.4 mm)\n",
      "epoch 14 0.16 (27.2 mm)\n",
      "epoch 15 0.16 (27.1 mm)\n",
      "epoch 16 0.16 (27.0 mm)\n",
      "epoch 17 0.16 (26.9 mm)\n",
      "epoch 18 0.16 (26.8 mm)\n",
      "epoch 19 0.16 (26.6 mm)\n",
      "epoch 20 0.16 (26.6 mm)\n",
      "epoch 21 0.16 (26.4 mm)\n",
      "epoch 22 0.16 (26.3 mm)\n",
      "epoch 23 0.16 (26.2 mm)\n",
      "epoch 24 0.16 (26.1 mm)\n",
      "epoch 25 0.16 (26.0 mm)\n",
      "epoch 26 0.16 (25.9 mm)\n",
      "epoch 27 0.16 (25.8 mm)\n",
      "epoch 28 0.16 (25.7 mm)\n",
      "epoch 29 0.15 (25.6 mm)\n",
      "epoch 30 0.15 (25.6 mm)\n",
      "epoch 31 0.15 (25.5 mm)\n",
      "epoch 32 0.15 (25.4 mm)\n",
      "epoch 33 0.15 (25.3 mm)\n",
      "epoch 34 0.15 (25.2 mm)\n",
      "epoch 35 0.15 (25.1 mm)\n",
      "epoch 36 0.15 (25.0 mm)\n",
      "epoch 37 0.15 (24.9 mm)\n",
      "epoch 38 0.15 (24.8 mm)\n",
      "epoch 39 0.15 (24.8 mm)\n",
      "epoch 40 0.15 (24.7 mm)\n",
      "epoch 41 0.15 (24.7 mm)\n",
      "epoch 42 0.15 (24.6 mm)\n",
      "epoch 43 0.15 (24.5 mm)\n",
      "epoch 44 0.15 (24.4 mm)\n",
      "epoch 45 0.15 (24.4 mm)\n",
      "epoch 46 0.15 (24.3 mm)\n",
      "epoch 47 0.15 (24.3 mm)\n",
      "epoch 48 0.15 (24.2 mm)\n",
      "epoch 49 0.15 (24.1 mm)\n",
      "epoch 50 0.15 (24.1 mm)\n",
      "epoch 51 0.14 (24.0 mm)\n",
      "epoch 52 0.14 (24.0 mm)\n",
      "epoch 53 0.14 (24.0 mm)\n",
      "epoch 54 0.14 (23.9 mm)\n",
      "epoch 55 0.14 (23.9 mm)\n",
      "epoch 56 0.14 (23.8 mm)\n",
      "epoch 57 0.14 (23.8 mm)\n",
      "epoch 58 0.14 (23.7 mm)\n",
      "epoch 59 0.14 (23.7 mm)\n",
      "epoch 60 0.14 (23.7 mm)\n",
      "epoch 61 0.14 (23.6 mm)\n",
      "epoch 62 0.14 (23.6 mm)\n",
      "epoch 63 0.14 (23.5 mm)\n",
      "epoch 64 0.14 (23.5 mm)\n",
      "epoch 65 0.14 (23.5 mm)\n",
      "epoch 66 0.14 (23.4 mm)\n",
      "epoch 67 0.14 (23.4 mm)\n",
      "epoch 68 0.14 (23.4 mm)\n",
      "epoch 69 0.14 (23.3 mm)\n",
      "epoch 70 0.14 (23.3 mm)\n",
      "epoch 71 0.14 (23.2 mm)\n",
      "epoch 72 0.14 (23.2 mm)\n",
      "epoch 73 0.14 (23.2 mm)\n",
      "epoch 74 0.14 (23.2 mm)\n",
      "epoch 75 0.14 (23.1 mm)\n",
      "epoch 76 0.14 (23.1 mm)\n",
      "epoch 77 0.14 (23.1 mm)\n",
      "epoch 78 0.14 (23.0 mm)\n",
      "epoch 79 0.14 (23.0 mm)\n",
      "epoch 80 0.14 (23.0 mm)\n",
      "epoch 81 0.14 (22.9 mm)\n",
      "epoch 82 0.14 (22.9 mm)\n",
      "epoch 83 0.14 (22.9 mm)\n",
      "epoch 84 0.14 (22.8 mm)\n",
      "epoch 85 0.14 (22.8 mm)\n",
      "epoch 86 0.14 (22.8 mm)\n",
      "epoch 87 0.14 (22.7 mm)\n",
      "epoch 88 0.14 (22.7 mm)\n",
      "epoch 89 0.14 (22.7 mm)\n",
      "epoch 90 0.14 (22.7 mm)\n",
      "epoch 91 0.14 (22.6 mm)\n",
      "epoch 92 0.14 (22.6 mm)\n",
      "epoch 93 0.14 (22.6 mm)\n",
      "epoch 94 0.14 (22.5 mm)\n",
      "epoch 95 0.14 (22.5 mm)\n",
      "epoch 96 0.14 (22.5 mm)\n",
      "epoch 97 0.14 (22.5 mm)\n",
      "epoch 98 0.14 (22.4 mm)\n",
      "epoch 99 0.13 (22.4 mm)\n",
      "epoch 100 0.13 (22.4 mm)\n",
      "epoch 101 0.13 (22.4 mm)\n",
      "epoch 102 0.13 (22.3 mm)\n",
      "epoch 103 0.13 (22.3 mm)\n",
      "epoch 104 0.13 (22.3 mm)\n",
      "epoch 105 0.13 (22.3 mm)\n",
      "epoch 106 0.13 (22.2 mm)\n",
      "epoch 107 0.13 (22.2 mm)\n",
      "epoch 108 0.13 (22.2 mm)\n",
      "epoch 109 0.13 (22.2 mm)\n",
      "epoch 110 0.13 (22.2 mm)\n",
      "epoch 111 0.13 (22.1 mm)\n",
      "epoch 112 0.13 (22.1 mm)\n",
      "epoch 113 0.13 (22.1 mm)\n",
      "epoch 114 0.13 (22.1 mm)\n",
      "epoch 115 0.13 (22.1 mm)\n",
      "epoch 116 0.13 (22.0 mm)\n",
      "epoch 117 0.13 (22.0 mm)\n",
      "epoch 118 0.13 (22.0 mm)\n",
      "epoch 119 0.13 (22.0 mm)\n",
      "epoch 120 0.13 (22.0 mm)\n",
      "epoch 121 0.13 (21.9 mm)\n",
      "epoch 122 0.13 (21.9 mm)\n",
      "epoch 123 0.13 (21.9 mm)\n",
      "epoch 124 0.13 (21.9 mm)\n",
      "epoch 125 0.13 (21.9 mm)\n",
      "epoch 126 0.13 (21.8 mm)\n",
      "epoch 127 0.13 (21.8 mm)\n",
      "epoch 128 0.13 (21.8 mm)\n",
      "epoch 129 0.13 (21.8 mm)\n",
      "epoch 130 0.13 (21.8 mm)\n",
      "epoch 131 0.13 (21.7 mm)\n",
      "epoch 132 0.13 (21.7 mm)\n",
      "epoch 133 0.13 (21.7 mm)\n",
      "epoch 134 0.13 (21.7 mm)\n",
      "epoch 135 0.13 (21.7 mm)\n",
      "epoch 136 0.13 (21.6 mm)\n",
      "epoch 137 0.13 (21.6 mm)\n",
      "epoch 138 0.13 (21.6 mm)\n",
      "epoch 139 0.13 (21.6 mm)\n",
      "epoch 140 0.13 (21.6 mm)\n",
      "epoch 141 0.13 (21.6 mm)\n",
      "epoch 142 0.13 (21.5 mm)\n",
      "epoch 143 0.13 (21.5 mm)\n",
      "epoch 144 0.13 (21.5 mm)\n",
      "epoch 145 0.13 (21.5 mm)\n",
      "epoch 146 0.13 (21.5 mm)\n",
      "epoch 147 0.13 (21.5 mm)\n",
      "epoch 148 0.13 (21.4 mm)\n",
      "epoch 149 0.13 (21.4 mm)\n",
      "epoch 150 0.13 (21.4 mm)\n",
      "epoch 151 0.13 (21.4 mm)\n",
      "epoch 152 0.13 (21.4 mm)\n",
      "epoch 153 0.13 (21.3 mm)\n",
      "epoch 154 0.13 (21.3 mm)\n",
      "epoch 155 0.13 (21.3 mm)\n",
      "epoch 156 0.13 (21.3 mm)\n",
      "epoch 157 0.13 (21.3 mm)\n",
      "epoch 158 0.13 (21.3 mm)\n",
      "epoch 159 0.13 (21.3 mm)\n",
      "epoch 160 0.13 (21.2 mm)\n",
      "epoch 161 0.13 (21.2 mm)\n",
      "epoch 162 0.13 (21.2 mm)\n",
      "epoch 163 0.13 (21.2 mm)\n",
      "epoch 164 0.13 (21.2 mm)\n",
      "epoch 165 0.13 (21.2 mm)\n",
      "epoch 166 0.13 (21.2 mm)\n",
      "epoch 167 0.13 (21.1 mm)\n",
      "epoch 168 0.13 (21.1 mm)\n",
      "epoch 169 0.13 (21.1 mm)\n",
      "epoch 170 0.13 (21.1 mm)\n",
      "epoch 171 0.13 (21.1 mm)\n",
      "epoch 172 0.13 (21.1 mm)\n",
      "epoch 173 0.13 (21.1 mm)\n",
      "epoch 174 0.13 (21.1 mm)\n",
      "epoch 175 0.13 (21.0 mm)\n",
      "epoch 176 0.13 (21.0 mm)\n",
      "epoch 177 0.13 (21.0 mm)\n",
      "epoch 178 0.13 (21.0 mm)\n",
      "epoch 179 0.13 (21.0 mm)\n",
      "epoch 180 0.13 (21.0 mm)\n",
      "epoch 181 0.13 (21.0 mm)\n",
      "epoch 182 0.13 (20.9 mm)\n",
      "epoch 183 0.13 (20.9 mm)\n",
      "epoch 184 0.13 (20.9 mm)\n",
      "epoch 185 0.13 (20.9 mm)\n",
      "epoch 186 0.13 (20.9 mm)\n",
      "epoch 187 0.13 (20.9 mm)\n",
      "epoch 188 0.13 (20.9 mm)\n",
      "epoch 189 0.13 (20.9 mm)\n",
      "epoch 190 0.13 (20.8 mm)\n",
      "epoch 191 0.13 (20.8 mm)\n",
      "epoch 192 0.13 (20.8 mm)\n",
      "epoch 193 0.13 (20.8 mm)\n",
      "epoch 194 0.13 (20.8 mm)\n",
      "epoch 195 0.13 (20.8 mm)\n",
      "epoch 196 0.13 (20.8 mm)\n",
      "epoch 197 0.13 (20.8 mm)\n",
      "epoch 198 0.12 (20.7 mm)\n",
      "epoch 199 0.12 (20.7 mm)\n",
      "epoch 200 0.12 (20.7 mm)\n",
      "epoch 201 0.12 (20.7 mm)\n",
      "epoch 202 0.12 (20.7 mm)\n",
      "epoch 203 0.12 (20.7 mm)\n",
      "epoch 204 0.12 (20.7 mm)\n",
      "epoch 205 0.12 (20.7 mm)\n",
      "epoch 206 0.12 (20.7 mm)\n",
      "epoch 207 0.12 (20.6 mm)\n",
      "epoch 208 0.12 (20.6 mm)\n",
      "epoch 209 0.12 (20.6 mm)\n",
      "epoch 210 0.12 (20.6 mm)\n",
      "epoch 211 0.12 (20.6 mm)\n",
      "epoch 212 0.12 (20.6 mm)\n",
      "epoch 213 0.12 (20.6 mm)\n",
      "epoch 214 0.12 (20.6 mm)\n",
      "epoch 215 0.12 (20.6 mm)\n",
      "epoch 216 0.12 (20.5 mm)\n",
      "epoch 217 0.12 (20.5 mm)\n",
      "epoch 218 0.12 (20.5 mm)\n",
      "epoch 219 0.12 (20.5 mm)\n",
      "epoch 220 0.12 (20.5 mm)\n",
      "epoch 221 0.12 (20.5 mm)\n",
      "epoch 222 0.12 (20.5 mm)\n",
      "epoch 223 0.12 (20.5 mm)\n",
      "epoch 224 0.12 (20.5 mm)\n",
      "epoch 225 0.12 (20.5 mm)\n",
      "epoch 226 0.12 (20.4 mm)\n",
      "epoch 227 0.12 (20.4 mm)\n",
      "epoch 228 0.12 (20.4 mm)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "sim_params_temp = deepcopy(sim_params)\n",
    "sim_params_temp[\"batch_size\"] = 1024\n",
    "sim_params_temp[\"n_sources\"] = (1,10)\n",
    "gen = generator(fwd, **sim_params_temp)\n",
    "for i in range(500):\n",
    "    X, y, _ = gen.__next__()\n",
    "    covs = [xx.T@xx for xx in X]\n",
    "    covs = np.stack([xx/abs(xx).max() for xx in covs], axis=0)\n",
    "    y_true = np.stack([(yy!=0)[0,:].astype(float) for yy in y], axis=0).astype(np.float32)\n",
    "    for _ in range(5):\n",
    "        loss = model.train_on_batch(covs, y_true)\n",
    "    print(f\"epoch {i} {loss:.2f} ({loss*max_dist:.1f} mm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop - progressing number of sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "\ttraining for 1 sources\n",
      "\ttraining for 2 sources\n",
      "\ttraining for 3 sources\n",
      "\ttraining for 4 sources\n",
      "\ttraining for 5 sources\n",
      "\ttraining for 6 sources\n",
      "\ttraining for 7 sources\n",
      "\ttraining for 8 sources\n",
      "\ttraining for 9 sources\n",
      "\ttraining for 10 sources\n",
      "\t\tLoss: 0.022, (3.7) mm\n",
      "\t\tLoss: 0.022, (3.7) mm\n",
      "\t\tLoss: 0.022, (3.7) mm\n",
      "\t\tLoss: 0.023, (3.8) mm\n",
      "\t\tLoss: 0.023, (3.8) mm\n",
      "\t\tLoss: 0.023, (3.8) mm\n",
      "\t\tLoss: 0.023, (3.8) mm\n",
      "\t\tLoss: 0.023, (3.8) mm\n",
      "\t\tLoss: 0.023, (3.8) mm\n",
      "\t\tLoss: 0.023, (3.8) mm\n",
      "epoch 1\n",
      "\ttraining for 1 sources\n",
      "\ttraining for 2 sources\n",
      "\ttraining for 3 sources\n",
      "\ttraining for 4 sources\n",
      "\ttraining for 5 sources\n",
      "\ttraining for 6 sources\n",
      "\ttraining for 7 sources\n",
      "\ttraining for 8 sources\n",
      "\ttraining for 9 sources\n",
      "\ttraining for 10 sources\n",
      "\t\tLoss: 0.023, (3.9) mm\n",
      "\t\tLoss: 0.023, (3.9) mm\n",
      "\t\tLoss: 0.023, (3.9) mm\n",
      "\t\tLoss: 0.023, (3.9) mm\n",
      "\t\tLoss: 0.024, (3.9) mm\n",
      "\t\tLoss: 0.024, (3.9) mm\n",
      "\t\tLoss: 0.024, (3.9) mm\n",
      "\t\tLoss: 0.024, (3.9) mm\n",
      "\t\tLoss: 0.024, (3.9) mm\n",
      "\t\tLoss: 0.024, (3.9) mm\n",
      "epoch 2\n",
      "\ttraining for 1 sources\n",
      "\ttraining for 2 sources\n",
      "\ttraining for 3 sources\n",
      "\ttraining for 4 sources\n",
      "\ttraining for 5 sources\n",
      "\ttraining for 6 sources\n",
      "\ttraining for 7 sources\n",
      "\ttraining for 8 sources\n",
      "\ttraining for 9 sources\n",
      "\ttraining for 10 sources\n",
      "\t\tLoss: 0.024, (3.9) mm\n",
      "\t\tLoss: 0.024, (3.9) mm\n",
      "\t\tLoss: 0.024, (4.0) mm\n",
      "\t\tLoss: 0.024, (4.0) mm\n",
      "\t\tLoss: 0.024, (4.0) mm\n",
      "\t\tLoss: 0.024, (4.0) mm\n",
      "\t\tLoss: 0.024, (4.0) mm\n",
      "\t\tLoss: 0.024, (4.0) mm\n",
      "\t\tLoss: 0.024, (4.0) mm\n",
      "\t\tLoss: 0.024, (4.0) mm\n",
      "epoch 3\n",
      "\ttraining for 1 sources\n",
      "\ttraining for 2 sources\n",
      "\ttraining for 3 sources\n",
      "\ttraining for 4 sources\n",
      "\ttraining for 5 sources\n",
      "\ttraining for 6 sources\n",
      "\ttraining for 7 sources\n",
      "\ttraining for 8 sources\n",
      "\ttraining for 9 sources\n",
      "\ttraining for 10 sources\n",
      "\t\tLoss: 0.024, (4.0) mm\n",
      "\t\tLoss: 0.024, (4.0) mm\n",
      "\t\tLoss: 0.024, (4.0) mm\n",
      "\t\tLoss: 0.024, (4.0) mm\n",
      "\t\tLoss: 0.024, (4.0) mm\n",
      "\t\tLoss: 0.024, (4.0) mm\n",
      "\t\tLoss: 0.024, (4.0) mm\n",
      "\t\tLoss: 0.024, (4.0) mm\n",
      "\t\tLoss: 0.024, (4.0) mm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Adjust parameters\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m---> 45\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(loss)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(loss)\u001b[38;5;241m*\u001b[39mmax_dist\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) mm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# model.save('rap_music_model.h5')\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:540\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m():\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[1;32m--> 540\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    541\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:233\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_flat\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: core\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[core\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    220\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flat tensor inputs and returns flat tensor outputs.\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;124;03m      available to be called because it has been garbage collected.\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m   expected_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcached_definition\u001b[49m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39minput_arg)\n\u001b[0;32m    234\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m expected_len:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSignature specifies \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m arguments, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(args)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Expected inputs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached_definition\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39minput_arg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Received inputs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Function Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:189\u001b[0m, in \u001b[0;36mAtomicFunction.cached_definition\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Cached FunctionDef (not guaranteed to be fresh).\"\"\"\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_definition \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_definition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefinition\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_definition\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:144\u001b[0m, in \u001b[0;36mAtomicFunction.definition\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefinition\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m function_pb2\u001b[38;5;241m.\u001b[39mFunctionDef:\n\u001b[0;32m    143\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Current FunctionDef in the Runtime.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_function_def\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "from copy import deepcopy\n",
    "\n",
    "sim_params[\"batch_size\"] = 1024\n",
    "n_sources = np.arange(10)+1\n",
    "\n",
    "epochs = 50\n",
    "epoch_distances = np.zeros(epochs)\n",
    "# Training loop within the RAP-MUSIC framework\n",
    "for epoch in range(epochs):  # Number of epochs\n",
    "    print(f\"epoch {epoch}\")\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    for n_source in n_sources:\n",
    "        print(f\"\\ttraining for {n_source} sources\")\n",
    "        sim_params[\"batch_size\"] = 1024 // n_source\n",
    "        sim_params[\"n_sources\"] = (n_source, n_source)\n",
    "        gen = generator(fwd, **sim_params)\n",
    "        X, true_dipoles, Y = generate_initial_data(gen) \n",
    "        current_data = deepcopy(X)\n",
    "        n_samples = len(true_dipoles)\n",
    "        estimated_dipole_idc = [list() for _ in range(n_samples)]\n",
    "\n",
    "        for i_iter in range(n_source):\n",
    "            # Compute Covariances\n",
    "            current_covs = np.stack([x@x.T for x in current_data], axis=0)\n",
    "            current_covs = np.stack([cov/abs(cov).max() for cov in current_covs], axis=0)\n",
    "            X_train.append(current_covs)\n",
    "            # Predict the sources using the model\n",
    "            estimated_sources = model.predict(current_covs, verbose=0)\n",
    "            predictions = model.predict(current_covs, verbose=0)\n",
    "            for i_sample in range(len(current_data)):\n",
    "                estimated_dipole_idc[i_sample].append( best_match_idx(pos, predictions[i_sample]*max_dist))\n",
    "\n",
    "            true_data_matched = np.zeros((n_samples, n_dipoles))\n",
    "            for i_sample in range(n_samples):\n",
    "                true_data_matched[i_sample, true_dipoles[i_sample]] = 1\n",
    "\n",
    "            Y_train.append(true_data_matched)\n",
    "            # Outproject the dipoles from the respective data\n",
    "            current_data = wrap_outproject_from_data(X, leadfield, estimated_dipole_idc, lam=1e-6)\n",
    "            \n",
    "    # Adjust parameters\n",
    "    for _ in range(10):\n",
    "        loss = model.train_on_batch(np.concatenate(X_train, axis=0), np.concatenate(Y_train, axis=0))\n",
    "        print(f\"\\t\\tLoss: {np.mean(loss):.3f}, ({np.mean(loss)*max_dist:.1f}) mm\")\n",
    "\n",
    "# Save the model\n",
    "# model.save('rap_music_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop - variable number of sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\tsample 1/64\n",
      "\tsample 2/64\n",
      "\tsample 3/64\n",
      "\tsample 4/64\n",
      "\tsample 5/64\n",
      "\tsample 6/64\n",
      "\tsample 7/64\n",
      "\tsample 8/64\n",
      "\tsample 9/64\n",
      "\tsample 10/64\n",
      "\tsample 11/64\n",
      "\tsample 12/64\n",
      "\tsample 13/64\n",
      "\tsample 14/64\n",
      "\tsample 15/64\n",
      "\tsample 16/64\n",
      "\tsample 17/64\n",
      "\tsample 18/64\n",
      "\tsample 19/64\n",
      "\tsample 20/64\n",
      "\tsample 21/64\n",
      "\tsample 22/64\n",
      "\tsample 23/64\n",
      "\tsample 24/64\n",
      "\tsample 25/64\n",
      "\tsample 26/64\n",
      "\tsample 27/64\n",
      "\tsample 28/64\n",
      "\tsample 29/64\n",
      "\tsample 30/64\n",
      "\tsample 31/64\n",
      "\tsample 32/64\n",
      "\tsample 33/64\n",
      "\tsample 34/64\n",
      "\tsample 35/64\n",
      "\tsample 36/64\n",
      "\tsample 37/64\n",
      "\tsample 38/64\n",
      "\tsample 39/64\n",
      "\tsample 40/64\n",
      "\tsample 41/64\n",
      "\tsample 42/64\n",
      "\tsample 43/64\n",
      "\tsample 44/64\n",
      "\tsample 45/64\n",
      "\tsample 46/64\n",
      "\tsample 47/64\n",
      "\tsample 48/64\n",
      "\tsample 49/64\n",
      "\tsample 50/64\n",
      "\tsample 51/64\n",
      "\tsample 52/64\n",
      "\tsample 53/64\n",
      "\tsample 54/64\n",
      "\tsample 55/64\n",
      "\tsample 56/64\n",
      "\tsample 57/64\n",
      "\tsample 58/64\n",
      "\tsample 59/64\n",
      "\tsample 60/64\n",
      "\tsample 61/64\n",
      "\tsample 62/64\n",
      "\tsample 63/64\n",
      "\tsample 64/64\n",
      "\t\t\tLoss: -0.06881794333457947\n",
      "\t\t\tLoss: -0.06870947033166885\n",
      "\t\t\tLoss: -0.06861625611782074\n",
      "\t\t\tLoss: -0.06854335963726044\n",
      "\t\t\tLoss: -0.06849012523889542\n",
      "\t\t\tLoss: -0.06841747462749481\n",
      "\t\t\tLoss: -0.0683523416519165\n",
      "\t\t\tLoss: -0.06831344217061996\n",
      "\t\t\tLoss: -0.06830259412527084\n",
      "\t\t\tLoss: -0.06829623878002167\n",
      "\t\t\tLoss: -0.0683012530207634\n",
      "\t\t\tLoss: -0.06832465529441833\n",
      "\t\t\tLoss: -0.06832828372716904\n",
      "\t\t\tLoss: -0.06834480911493301\n",
      "\t\t\tLoss: -0.0683877244591713\n",
      "\t\t\tLoss: -0.06843051314353943\n",
      "\t\t\tLoss: -0.06850318610668182\n",
      "\t\t\tLoss: -0.06860573589801788\n",
      "\t\t\tLoss: -0.06872954964637756\n",
      "\t\t\tLoss: -0.06888020038604736\n",
      "\t\t\tLoss: -0.06905660033226013\n",
      "\t\t\tLoss: -0.06924380362033844\n",
      "\t\t\tLoss: -0.06945602595806122\n",
      "\t\t\tLoss: -0.06969122588634491\n",
      "\t\t\tLoss: -0.06994854658842087\n",
      "\t\t\tLoss: -0.07022790610790253\n",
      "\t\t\tLoss: -0.07053597271442413\n",
      "\t\t\tLoss: -0.07084185630083084\n",
      "\t\t\tLoss: -0.07114532589912415\n",
      "\t\t\tLoss: -0.07144621759653091\n",
      "\t\t\tLoss: -0.07176576554775238\n",
      "\t\t\tLoss: -0.07211770117282867\n",
      "\t\t\tLoss: -0.07247951626777649\n",
      "\t\t\tLoss: -0.07285065203905106\n",
      "\t\t\tLoss: -0.07323035597801208\n",
      "\t\t\tLoss: -0.07362528890371323\n",
      "\t\t\tLoss: -0.07402059435844421\n",
      "\t\t\tLoss: -0.07442289590835571\n",
      "\t\t\tLoss: -0.07484575361013412\n",
      "\t\t\tLoss: -0.07528859376907349\n",
      "\t\t\tLoss: -0.07575078308582306\n",
      "\t\t\tLoss: -0.07622450590133667\n",
      "\t\t\tLoss: -0.07668127119541168\n",
      "\t\t\tLoss: -0.07715592533349991\n",
      "\t\t\tLoss: -0.07764091342687607\n",
      "\t\t\tLoss: -0.07813572138547897\n",
      "\t\t\tLoss: -0.07863302528858185\n",
      "\t\t\tLoss: -0.07915329188108444\n",
      "\t\t\tLoss: -0.07966839522123337\n",
      "\t\t\tLoss: -0.08020558208227158\n",
      "\t\t\tLoss: -0.08075061440467834\n",
      "\t\t\tLoss: -0.08131004124879837\n",
      "\t\t\tLoss: -0.08188344538211823\n",
      "\t\t\tLoss: -0.08246367424726486\n",
      "\t\t\tLoss: -0.08305706828832626\n",
      "\t\t\tLoss: -0.08365659415721893\n",
      "\t\t\tLoss: -0.08426862955093384\n",
      "\t\t\tLoss: -0.08487953245639801\n",
      "\t\t\tLoss: -0.08549568057060242\n",
      "\t\t\tLoss: -0.08610357344150543\n",
      "\t\t\tLoss: -0.08674268424510956\n",
      "\t\t\tLoss: -0.08737307786941528\n",
      "\t\t\tLoss: -0.08800771832466125\n",
      "\t\t\tLoss: -0.08863333612680435\n",
      "\t\t\tLoss: -0.08925633132457733\n",
      "\t\t\tLoss: -0.08988957107067108\n",
      "\t\t\tLoss: -0.09051983058452606\n",
      "\t\t\tLoss: -0.09117278456687927\n",
      "\t\t\tLoss: -0.09182874858379364\n",
      "\t\t\tLoss: -0.09248757362365723\n",
      "\t\t\tLoss: -0.09314905107021332\n",
      "\t\t\tLoss: -0.09380665421485901\n",
      "\t\t\tLoss: -0.09446025639772415\n",
      "\t\t\tLoss: -0.09511612355709076\n",
      "\t\t\tLoss: -0.09576773643493652\n",
      "\t\t\tLoss: -0.09640873968601227\n",
      "\t\t\tLoss: -0.09705789387226105\n",
      "\t\t\tLoss: -0.09772126376628876\n",
      "\t\t\tLoss: -0.09838610887527466\n",
      "\t\t\tLoss: -0.099052295088768\n",
      "\t\t\tLoss: -0.09971967339515686\n",
      "\t\t\tLoss: -0.10038195550441742\n",
      "\t\t\tLoss: -0.10104523599147797\n",
      "\t\t\tLoss: -0.10170324146747589\n",
      "\t\t\tLoss: -0.10237420350313187\n",
      "\t\t\tLoss: -0.10303959995508194\n",
      "\t\t\tLoss: -0.10370529443025589\n",
      "\t\t\tLoss: -0.1043829470872879\n",
      "\t\t\tLoss: -0.10504759848117828\n",
      "\t\t\tLoss: -0.10571195185184479\n",
      "\t\t\tLoss: -0.10636568069458008\n",
      "\t\t\tLoss: -0.10702010989189148\n",
      "\t\t\tLoss: -0.10767947882413864\n",
      "\t\t\tLoss: -0.10832685232162476\n",
      "\t\t\tLoss: -0.10897499322891235\n",
      "\t\t\tLoss: -0.10963968932628632\n",
      "\t\t\tLoss: -0.11028586328029633\n",
      "\t\t\tLoss: -0.11094433069229126\n",
      "\t\t\tLoss: -0.11160735785961151\n",
      "\t\t\tLoss: -0.11225789040327072\n",
      "\t\t\tLoss: -0.11290831863880157\n",
      "\t\t\tLoss: -0.11356301605701447\n",
      "\t\t\tLoss: -0.1142052561044693\n",
      "\t\t\tLoss: -0.11484698951244354\n",
      "\t\t\tLoss: -0.11548715829849243\n",
      "\t\t\tLoss: -0.11610911786556244\n",
      "\t\t\tLoss: -0.11674179881811142\n",
      "\t\t\tLoss: -0.11737856268882751\n",
      "\t\t\tLoss: -0.11799711734056473\n",
      "\t\t\tLoss: -0.11861465871334076\n",
      "\t\t\tLoss: -0.11924749612808228\n",
      "\t\t\tLoss: -0.1198619082570076\n",
      "\t\t\tLoss: -0.1204918622970581\n",
      "\t\t\tLoss: -0.12112534046173096\n",
      "\t\t\tLoss: -0.12175705283880234\n",
      "\t\t\tLoss: -0.12237027287483215\n",
      "\t\t\tLoss: -0.12300440669059753\n",
      "\t\t\tLoss: -0.12361498177051544\n",
      "\t\t\tLoss: -0.12422975897789001\n",
      "\t\t\tLoss: -0.12485915422439575\n",
      "\t\t\tLoss: -0.12546992301940918\n",
      "\t\t\tLoss: -0.12609519064426422\n",
      "\t\t\tLoss: -0.1267184168100357\n",
      "\t\t\tLoss: -0.12732364237308502\n",
      "\t\t\tLoss: -0.12792721390724182\n",
      "\t\t\tLoss: -0.12854501605033875\n",
      "\t\t\tLoss: -0.12914982438087463\n",
      "\t\t\tLoss: -0.12975254654884338\n",
      "\t\t\tLoss: -0.13036929070949554\n",
      "\t\t\tLoss: -0.13096243143081665\n",
      "\t\t\tLoss: -0.13156971335411072\n",
      "\t\t\tLoss: -0.13215896487236023\n",
      "\t\t\tLoss: -0.1327514946460724\n",
      "\t\t\tLoss: -0.13334718346595764\n",
      "\t\t\tLoss: -0.13393011689186096\n",
      "\t\t\tLoss: -0.1345108449459076\n",
      "\t\t\tLoss: -0.13508406281471252\n",
      "\t\t\tLoss: -0.13565507531166077\n",
      "\t\t\tLoss: -0.136208176612854\n",
      "\t\t\tLoss: -0.13678526878356934\n",
      "\t\t\tLoss: -0.13732346892356873\n",
      "\t\t\tLoss: -0.13788028061389923\n",
      "\t\t\tLoss: -0.13841387629508972\n",
      "\t\t\tLoss: -0.13897651433944702\n",
      "\t\t\tLoss: -0.13951638340950012\n",
      "\t\t\tLoss: -0.14005422592163086\n",
      "\t\t\tLoss: -0.14061538875102997\n",
      "\t\t\tLoss: -0.14112764596939087\n",
      "\t\t\tLoss: -0.14168453216552734\n",
      "\t\t\tLoss: -0.14220932126045227\n",
      "\t\t\tLoss: -0.14273199439048767\n",
      "\t\t\tLoss: -0.14328768849372864\n",
      "\t\t\tLoss: -0.14379079639911652\n",
      "\t\t\tLoss: -0.1443326771259308\n",
      "\t\t\tLoss: -0.14488214254379272\n",
      "\t\t\tLoss: -0.14539407193660736\n",
      "\t\t\tLoss: -0.14594420790672302\n",
      "\t\t\tLoss: -0.14648710191249847\n",
      "\t\t\tLoss: -0.14701272547245026\n",
      "\t\t\tLoss: -0.14756618440151215\n",
      "\t\t\tLoss: -0.1480925977230072\n",
      "\t\t\tLoss: -0.14861656725406647\n",
      "\t\t\tLoss: -0.14915822446346283\n",
      "\t\t\tLoss: -0.14968308806419373\n",
      "\t\t\tLoss: -0.15020069479942322\n",
      "\t\t\tLoss: -0.1507258415222168\n",
      "\t\t\tLoss: -0.15125377476215363\n",
      "\t\t\tLoss: -0.151755228638649\n",
      "\t\t\tLoss: -0.15227878093719482\n",
      "\t\t\tLoss: -0.15279527008533478\n",
      "\t\t\tLoss: -0.153304785490036\n",
      "\t\t\tLoss: -0.1538168489933014\n",
      "\t\t\tLoss: -0.1543169617652893\n",
      "\t\t\tLoss: -0.15482421219348907\n",
      "\t\t\tLoss: -0.15532854199409485\n",
      "\t\t\tLoss: -0.15582957863807678\n",
      "\t\t\tLoss: -0.1563425064086914\n",
      "\t\t\tLoss: -0.15682612359523773\n",
      "\t\t\tLoss: -0.15731358528137207\n",
      "\t\t\tLoss: -0.15780311822891235\n",
      "\t\t\tLoss: -0.15828536450862885\n",
      "\t\t\tLoss: -0.15878039598464966\n",
      "\t\t\tLoss: -0.1592501699924469\n",
      "\t\t\tLoss: -0.1597408950328827\n",
      "\t\t\tLoss: -0.1602204293012619\n",
      "\t\t\tLoss: -0.16067031025886536\n",
      "\t\t\tLoss: -0.16114123165607452\n",
      "\t\t\tLoss: -0.16158704459667206\n",
      "\t\t\tLoss: -0.16203121840953827\n",
      "\t\t\tLoss: -0.16250567138195038\n",
      "\t\t\tLoss: -0.16294589638710022\n",
      "\t\t\tLoss: -0.16338446736335754\n",
      "\t\t\tLoss: -0.16384394466876984\n",
      "\t\t\tLoss: -0.16427406668663025\n",
      "\t\t\tLoss: -0.16471625864505768\n",
      "\t\t\tLoss: -0.16516560316085815\n",
      "\t\t\tLoss: -0.165603905916214\n",
      "\t\t\tLoss: -0.16605406999588013\n",
      "\t\t\tLoss: -0.16651582717895508\n",
      "\t\t\tLoss: -0.16695302724838257\n",
      "\t\t\tLoss: -0.16740180552005768\n",
      "\t\t\tLoss: -0.16784408688545227\n",
      "\t\t\tLoss: -0.16829310357570648\n",
      "\t\t\tLoss: -0.1687488704919815\n",
      "\t\t\tLoss: -0.16920220851898193\n",
      "\t\t\tLoss: -0.16962720453739166\n",
      "\t\t\tLoss: -0.1700730323791504\n",
      "\t\t\tLoss: -0.1705000102519989\n",
      "\t\t\tLoss: -0.17092537879943848\n",
      "\t\t\tLoss: -0.17135749757289886\n",
      "\t\t\tLoss: -0.17177435755729675\n",
      "\t\t\tLoss: -0.17220699787139893\n",
      "\t\t\tLoss: -0.17264260351657867\n",
      "\t\t\tLoss: -0.17306354641914368\n",
      "\t\t\tLoss: -0.17349570989608765\n",
      "\t\t\tLoss: -0.17392152547836304\n",
      "\t\t\tLoss: -0.1743541955947876\n",
      "\t\t\tLoss: -0.17478534579277039\n",
      "\t\t\tLoss: -0.17521479725837708\n",
      "\t\t\tLoss: -0.17565105855464935\n",
      "\t\t\tLoss: -0.1760639101266861\n",
      "\t\t\tLoss: -0.17647932469844818\n",
      "\t\t\tLoss: -0.17690172791481018\n",
      "\t\t\tLoss: -0.17730966210365295\n",
      "\t\t\tLoss: -0.17773720622062683\n",
      "\t\t\tLoss: -0.17814591526985168\n",
      "\t\t\tLoss: -0.17855292558670044\n",
      "\t\t\tLoss: -0.17897531390190125\n",
      "\t\t\tLoss: -0.17937487363815308\n",
      "\t\t\tLoss: -0.1797896921634674\n",
      "\t\t\tLoss: -0.1801985800266266\n",
      "\t\t\tLoss: -0.18062257766723633\n",
      "\t\t\tLoss: -0.18102386593818665\n",
      "\t\t\tLoss: -0.1814444363117218\n",
      "\t\t\tLoss: -0.18185073137283325\n",
      "\t\t\tLoss: -0.18227186799049377\n",
      "\t\t\tLoss: -0.1826743185520172\n",
      "\t\t\tLoss: -0.18307027220726013\n",
      "\t\t\tLoss: -0.18345987796783447\n",
      "\t\t\tLoss: -0.18385624885559082\n",
      "\t\t\tLoss: -0.1842564046382904\n",
      "\t\t\tLoss: -0.18467208743095398\n",
      "\t\t\tLoss: -0.1850651204586029\n",
      "\t\t\tLoss: -0.185460165143013\n",
      "\t\t\tLoss: -0.18586644530296326\n",
      "\t\t\tLoss: -0.18625932931900024\n",
      "\t\t\tLoss: -0.18665429949760437\n",
      "\t\t\tLoss: -0.18704754114151\n",
      "\t\t\tLoss: -0.18743160367012024\n",
      "\t\t\tLoss: -0.18783441185951233\n",
      "\t\t\tLoss: -0.1882191151380539\n",
      "\t\t\tLoss: -0.18860656023025513\n",
      "\t\t\tLoss: -0.18900072574615479\n",
      "\t\t\tLoss: -0.18939314782619476\n",
      "\t\t\tLoss: -0.18977200984954834\n",
      "\t\t\tLoss: -0.1901455819606781\n",
      "\t\t\tLoss: -0.1905335634946823\n",
      "\t\t\tLoss: -0.1909080147743225\n",
      "\t\t\tLoss: -0.19128909707069397\n",
      "\t\t\tLoss: -0.1916607916355133\n",
      "\t\t\tLoss: -0.19203487038612366\n",
      "\t\t\tLoss: -0.19240757822990417\n",
      "\t\t\tLoss: -0.19277101755142212\n",
      "\t\t\tLoss: -0.19313301146030426\n",
      "\t\t\tLoss: -0.19349360466003418\n",
      "\t\t\tLoss: -0.1938646137714386\n",
      "\t\t\tLoss: -0.19422250986099243\n",
      "\t\t\tLoss: -0.19457504153251648\n",
      "\t\t\tLoss: -0.19493791460990906\n",
      "\t\t\tLoss: -0.1952800303697586\n",
      "\t\t\tLoss: -0.1956363171339035\n",
      "\t\t\tLoss: -0.19597959518432617\n",
      "\t\t\tLoss: -0.19632546603679657\n",
      "\t\t\tLoss: -0.19666612148284912\n",
      "\t\t\tLoss: -0.19700929522514343\n",
      "\t\t\tLoss: -0.1973317265510559\n",
      "\t\t\tLoss: -0.1976795196533203\n",
      "\t\t\tLoss: -0.19800221920013428\n",
      "\t\t\tLoss: -0.19834980368614197\n",
      "\t\t\tLoss: -0.19866859912872314\n",
      "\t\t\tLoss: -0.19900676608085632\n",
      "\t\t\tLoss: -0.1993447244167328\n",
      "\t\t\tLoss: -0.19963958859443665\n",
      "\t\t\tLoss: -0.19998198747634888\n",
      "\t\t\tLoss: -0.20030799508094788\n",
      "\t\t\tLoss: -0.20062220096588135\n",
      "\t\t\tLoss: -0.20097264647483826\n",
      "\t\t\tLoss: -0.20126892626285553\n",
      "\t\t\tLoss: -0.2016095519065857\n",
      "\t\t\tLoss: -0.20193038880825043\n",
      "\t\t\tLoss: -0.20222723484039307\n",
      "\t\t\tLoss: -0.2025715708732605\n",
      "\t\t\tLoss: -0.2028851956129074\n",
      "\t\t\tLoss: -0.20319008827209473\n",
      "\t\t\tLoss: -0.20353087782859802\n",
      "\t\t\tLoss: -0.20383727550506592\n",
      "\t\t\tLoss: -0.20413514971733093\n",
      "\t\t\tLoss: -0.20445767045021057\n",
      "\t\t\tLoss: -0.20474600791931152\n",
      "\t\t\tLoss: -0.20504802465438843\n",
      "Epoch 2/50\n",
      "\tsample 1/64\n",
      "\tsample 2/64\n",
      "\tsample 3/64\n",
      "\tsample 4/64\n",
      "\tsample 5/64\n",
      "\tsample 6/64\n",
      "\tsample 7/64\n",
      "\tsample 8/64\n",
      "\tsample 9/64\n",
      "\tsample 10/64\n",
      "\tsample 11/64\n",
      "\tsample 12/64\n",
      "\tsample 13/64\n",
      "\tsample 14/64\n",
      "\tsample 15/64\n",
      "\tsample 16/64\n",
      "\tsample 17/64\n",
      "\tsample 18/64\n",
      "\tsample 19/64\n",
      "\tsample 20/64\n",
      "\tsample 21/64\n",
      "\tsample 22/64\n",
      "\tsample 23/64\n",
      "\tsample 24/64\n",
      "\tsample 25/64\n",
      "\tsample 26/64\n",
      "\tsample 27/64\n",
      "\tsample 28/64\n",
      "\tsample 29/64\n",
      "\tsample 30/64\n",
      "\tsample 31/64\n",
      "\tsample 32/64\n",
      "\tsample 33/64\n",
      "\tsample 34/64\n",
      "\tsample 35/64\n",
      "\tsample 36/64\n",
      "\tsample 37/64\n",
      "\tsample 38/64\n",
      "\tsample 39/64\n",
      "\tsample 40/64\n",
      "\tsample 41/64\n",
      "\tsample 42/64\n",
      "\tsample 43/64\n",
      "\tsample 44/64\n",
      "\tsample 45/64\n",
      "\tsample 46/64\n",
      "\tsample 47/64\n",
      "\tsample 48/64\n",
      "\tsample 49/64\n",
      "\tsample 50/64\n",
      "\tsample 51/64\n",
      "\tsample 52/64\n",
      "\tsample 53/64\n",
      "\tsample 54/64\n",
      "\tsample 55/64\n",
      "\tsample 56/64\n",
      "\tsample 57/64\n",
      "\tsample 58/64\n",
      "\tsample 59/64\n",
      "\tsample 60/64\n",
      "\tsample 61/64\n",
      "\tsample 62/64\n",
      "\tsample 63/64\n",
      "\tsample 64/64\n",
      "\t\t\tLoss: -0.2046927809715271\n",
      "\t\t\tLoss: -0.20436811447143555\n",
      "\t\t\tLoss: -0.20405355095863342\n",
      "\t\t\tLoss: -0.20376262068748474\n",
      "\t\t\tLoss: -0.20347166061401367\n",
      "\t\t\tLoss: -0.20320022106170654\n",
      "\t\t\tLoss: -0.20291849970817566\n",
      "\t\t\tLoss: -0.20262253284454346\n",
      "\t\t\tLoss: -0.20233309268951416\n",
      "\t\t\tLoss: -0.2020590901374817\n",
      "\t\t\tLoss: -0.20180551707744598\n",
      "\t\t\tLoss: -0.20157012343406677\n",
      "\t\t\tLoss: -0.20135843753814697\n",
      "\t\t\tLoss: -0.20116204023361206\n",
      "\t\t\tLoss: -0.20097273588180542\n",
      "\t\t\tLoss: -0.20080065727233887\n",
      "\t\t\tLoss: -0.20064517855644226\n",
      "\t\t\tLoss: -0.2004844844341278\n",
      "\t\t\tLoss: -0.2003358155488968\n",
      "\t\t\tLoss: -0.2002127319574356\n",
      "\t\t\tLoss: -0.20010769367218018\n",
      "\t\t\tLoss: -0.2000202238559723\n",
      "\t\t\tLoss: -0.19994986057281494\n",
      "\t\t\tLoss: -0.19988533854484558\n",
      "\t\t\tLoss: -0.19983655214309692\n",
      "\t\t\tLoss: -0.19979581236839294\n",
      "\t\t\tLoss: -0.19977673888206482\n",
      "\t\t\tLoss: -0.1997716724872589\n",
      "\t\t\tLoss: -0.19977661967277527\n",
      "\t\t\tLoss: -0.19979795813560486\n",
      "\t\t\tLoss: -0.19982823729515076\n",
      "\t\t\tLoss: -0.19986698031425476\n",
      "\t\t\tLoss: -0.1999104619026184\n",
      "\t\t\tLoss: -0.19996514916419983\n",
      "\t\t\tLoss: -0.20003420114517212\n",
      "\t\t\tLoss: -0.20011714100837708\n",
      "\t\t\tLoss: -0.20021021366119385\n",
      "\t\t\tLoss: -0.20030619204044342\n",
      "\t\t\tLoss: -0.2004013955593109\n",
      "\t\t\tLoss: -0.2005023956298828\n",
      "\t\t\tLoss: -0.2006089687347412\n",
      "\t\t\tLoss: -0.20072084665298462\n",
      "\t\t\tLoss: -0.20083439350128174\n",
      "\t\t\tLoss: -0.20094937086105347\n",
      "\t\t\tLoss: -0.20106901228427887\n",
      "\t\t\tLoss: -0.20119652152061462\n",
      "\t\t\tLoss: -0.20133166015148163\n",
      "\t\t\tLoss: -0.20146748423576355\n",
      "\t\t\tLoss: -0.20160718262195587\n",
      "\t\t\tLoss: -0.20175060629844666\n",
      "\t\t\tLoss: -0.2019043117761612\n",
      "\t\t\tLoss: -0.20206469297409058\n",
      "\t\t\tLoss: -0.20223501324653625\n",
      "\t\t\tLoss: -0.2024083435535431\n",
      "\t\t\tLoss: -0.20257799327373505\n",
      "\t\t\tLoss: -0.202743798494339\n",
      "\t\t\tLoss: -0.20290905237197876\n",
      "\t\t\tLoss: -0.20307691395282745\n",
      "\t\t\tLoss: -0.2032473236322403\n",
      "\t\t\tLoss: -0.20343664288520813\n",
      "\t\t\tLoss: -0.2036249041557312\n",
      "\t\t\tLoss: -0.20381863415241241\n",
      "\t\t\tLoss: -0.20401445031166077\n",
      "\t\t\tLoss: -0.20420892536640167\n",
      "\t\t\tLoss: -0.20440861582756042\n",
      "\t\t\tLoss: -0.2046133279800415\n",
      "\t\t\tLoss: -0.20481981337070465\n",
      "\t\t\tLoss: -0.20502793788909912\n",
      "\t\t\tLoss: -0.20524084568023682\n",
      "\t\t\tLoss: -0.20545201003551483\n",
      "\t\t\tLoss: -0.20565810799598694\n",
      "\t\t\tLoss: -0.20587530732154846\n",
      "\t\t\tLoss: -0.20609375834465027\n",
      "\t\t\tLoss: -0.20631346106529236\n",
      "\t\t\tLoss: -0.20652782917022705\n",
      "\t\t\tLoss: -0.20674654841423035\n",
      "\t\t\tLoss: -0.20696309208869934\n",
      "\t\t\tLoss: -0.20717746019363403\n",
      "\t\t\tLoss: -0.20738957822322845\n",
      "\t\t\tLoss: -0.2075994312763214\n",
      "\t\t\tLoss: -0.20780697464942932\n",
      "\t\t\tLoss: -0.20800897479057312\n",
      "\t\t\tLoss: -0.20821182429790497\n",
      "\t\t\tLoss: -0.2084154337644577\n",
      "\t\t\tLoss: -0.20862294733524323\n",
      "\t\t\tLoss: -0.20883426070213318\n",
      "\t\t\tLoss: -0.20905250310897827\n",
      "\t\t\tLoss: -0.2092745006084442\n",
      "\t\t\tLoss: -0.20949384570121765\n",
      "\t\t\tLoss: -0.2097136676311493\n",
      "\t\t\tLoss: -0.20993709564208984\n",
      "\t\t\tLoss: -0.21016094088554382\n",
      "\t\t\tLoss: -0.21038511395454407\n",
      "\t\t\tLoss: -0.21060656011104584\n",
      "\t\t\tLoss: -0.21082830429077148\n",
      "\t\t\tLoss: -0.21104413270950317\n",
      "\t\t\tLoss: -0.21126019954681396\n",
      "\t\t\tLoss: -0.21147657930850983\n",
      "\t\t\tLoss: -0.2116931825876236\n",
      "\t\t\tLoss: -0.21190688014030457\n",
      "\t\t\tLoss: -0.21211767196655273\n",
      "\t\t\tLoss: -0.21232862770557404\n",
      "\t\t\tLoss: -0.21253976225852966\n",
      "\t\t\tLoss: -0.21274793148040771\n",
      "\t\t\tLoss: -0.21295621991157532\n",
      "\t\t\tLoss: -0.21316154301166534\n",
      "\t\t\tLoss: -0.213363915681839\n",
      "\t\t\tLoss: -0.21356329321861267\n",
      "\t\t\tLoss: -0.21376270055770874\n",
      "\t\t\tLoss: -0.21396219730377197\n",
      "\t\t\tLoss: -0.21415865421295166\n",
      "\t\t\tLoss: -0.21435514092445374\n",
      "\t\t\tLoss: -0.21455159783363342\n",
      "\t\t\tLoss: -0.2147480547428131\n",
      "\t\t\tLoss: -0.21494147181510925\n",
      "\t\t\tLoss: -0.21513482928276062\n",
      "\t\t\tLoss: -0.215328186750412\n",
      "\t\t\tLoss: -0.215521439909935\n",
      "\t\t\tLoss: -0.21571463346481323\n",
      "\t\t\tLoss: -0.21590769290924072\n",
      "\t\t\tLoss: -0.21610066294670105\n",
      "\t\t\tLoss: -0.2162935435771942\n",
      "\t\t\tLoss: -0.21648924052715302\n",
      "\t\t\tLoss: -0.21668481826782227\n",
      "\t\t\tLoss: -0.21688023209571838\n",
      "\t\t\tLoss: -0.21707549691200256\n",
      "\t\t\tLoss: -0.21726757287979126\n",
      "\t\t\tLoss: -0.21745947003364563\n",
      "\t\t\tLoss: -0.21765121817588806\n",
      "\t\t\tLoss: -0.2178427278995514\n",
      "\t\t\tLoss: -0.2180340588092804\n",
      "\t\t\tLoss: -0.21822220087051392\n",
      "\t\t\tLoss: -0.21840721368789673\n",
      "\t\t\tLoss: -0.21859200298786163\n",
      "\t\t\tLoss: -0.21877658367156982\n",
      "\t\t\tLoss: -0.2189638465642929\n",
      "\t\t\tLoss: -0.21915671229362488\n",
      "\t\t\tLoss: -0.21934932470321655\n",
      "\t\t\tLoss: -0.21954163908958435\n",
      "\t\t\tLoss: -0.21973659098148346\n",
      "\t\t\tLoss: -0.2199370563030243\n",
      "\t\t\tLoss: -0.22013717889785767\n",
      "\t\t\tLoss: -0.22033700346946716\n",
      "\t\t\tLoss: -0.2205365002155304\n",
      "\t\t\tLoss: -0.22073854506015778\n",
      "\t\t\tLoss: -0.2209402322769165\n",
      "\t\t\tLoss: -0.22114157676696777\n",
      "\t\t\tLoss: -0.22134257853031158\n",
      "\t\t\tLoss: -0.22154033184051514\n",
      "\t\t\tLoss: -0.22174058854579926\n",
      "\t\t\tLoss: -0.22194337844848633\n",
      "\t\t\tLoss: -0.22214291989803314\n",
      "\t\t\tLoss: -0.22234207391738892\n",
      "\t\t\tLoss: -0.22254085540771484\n",
      "\t\t\tLoss: -0.22274208068847656\n",
      "\t\t\tLoss: -0.22293725609779358\n",
      "\t\t\tLoss: -0.2231292426586151\n",
      "\t\t\tLoss: -0.2233208268880844\n",
      "\t\t\tLoss: -0.22351206839084625\n",
      "\t\t\tLoss: -0.2237057387828827\n",
      "\t\t\tLoss: -0.22390183806419373\n",
      "\t\t\tLoss: -0.22409752011299133\n",
      "\t\t\tLoss: -0.2242928296327591\n",
      "\t\t\tLoss: -0.2244848906993866\n",
      "\t\t\tLoss: -0.22467941045761108\n",
      "\t\t\tLoss: -0.2248706966638565\n",
      "\t\t\tLoss: -0.2250615954399109\n",
      "\t\t\tLoss: -0.22525209188461304\n",
      "\t\t\tLoss: -0.2254449725151062\n",
      "\t\t\tLoss: -0.22563186287879944\n",
      "\t\t\tLoss: -0.22581833600997925\n",
      "\t\t\tLoss: -0.22600999474525452\n",
      "\t\t\tLoss: -0.2262040078639984\n",
      "\t\t\tLoss: -0.22640308737754822\n",
      "\t\t\tLoss: -0.2266017496585846\n",
      "\t\t\tLoss: -0.22679448127746582\n",
      "\t\t\tLoss: -0.22698675096035004\n",
      "\t\t\tLoss: -0.22717857360839844\n",
      "\t\t\tLoss: -0.2273699939250946\n",
      "\t\t\tLoss: -0.22756095230579376\n",
      "\t\t\tLoss: -0.2277514934539795\n",
      "\t\t\tLoss: -0.227941632270813\n",
      "\t\t\tLoss: -0.22813130915164948\n",
      "\t\t\tLoss: -0.22832328081130981\n",
      "\t\t\tLoss: -0.2285175621509552\n",
      "\t\t\tLoss: -0.22871136665344238\n",
      "\t\t\tLoss: -0.22890476882457733\n",
      "\t\t\tLoss: -0.22909769415855408\n",
      "\t\t\tLoss: -0.229290172457695\n",
      "\t\t\tLoss: -0.22948220372200012\n",
      "\t\t\tLoss: -0.2296764850616455\n",
      "\t\t\tLoss: -0.22986763715744019\n",
      "\t\t\tLoss: -0.2300529181957245\n",
      "\t\t\tLoss: -0.23023776710033417\n",
      "\t\t\tLoss: -0.23041948676109314\n",
      "\t\t\tLoss: -0.23060081899166107\n",
      "\t\t\tLoss: -0.23078171908855438\n",
      "\t\t\tLoss: -0.23096218705177307\n",
      "\t\t\tLoss: -0.23114225268363953\n",
      "\t\t\tLoss: -0.23132984340190887\n",
      "\t\t\tLoss: -0.2315170168876648\n",
      "\t\t\tLoss: -0.2317037731409073\n",
      "\t\t\tLoss: -0.23189003765583038\n",
      "\t\t\tLoss: -0.23207589983940125\n",
      "\t\t\tLoss: -0.2322613000869751\n",
      "\t\t\tLoss: -0.23244094848632812\n",
      "\t\t\tLoss: -0.23262019455432892\n",
      "\t\t\tLoss: -0.2328016608953476\n",
      "\t\t\tLoss: -0.23298268020153046\n",
      "\t\t\tLoss: -0.2331632375717163\n",
      "\t\t\tLoss: -0.23334340751171112\n",
      "\t\t\tLoss: -0.2335204929113388\n",
      "\t\t\tLoss: -0.23369717597961426\n",
      "\t\t\tLoss: -0.2338734269142151\n",
      "\t\t\tLoss: -0.2340492457151413\n",
      "\t\t\tLoss: -0.23422466218471527\n",
      "\t\t\tLoss: -0.23439964652061462\n",
      "\t\t\tLoss: -0.23457422852516174\n",
      "\t\t\tLoss: -0.23475097119808197\n",
      "\t\t\tLoss: -0.23492729663848877\n",
      "\t\t\tLoss: -0.2351083755493164\n",
      "\t\t\tLoss: -0.23528644442558289\n",
      "\t\t\tLoss: -0.23546148836612701\n",
      "\t\t\tLoss: -0.2356361448764801\n",
      "\t\t\tLoss: -0.23581290245056152\n",
      "\t\t\tLoss: -0.2359892576932907\n",
      "\t\t\tLoss: -0.23616519570350647\n",
      "\t\t\tLoss: -0.2363407015800476\n",
      "\t\t\tLoss: -0.2365158051252365\n",
      "\t\t\tLoss: -0.23669305443763733\n",
      "\t\t\tLoss: -0.23686981201171875\n",
      "\t\t\tLoss: -0.23704619705677032\n",
      "\t\t\tLoss: -0.23722214996814728\n",
      "\t\t\tLoss: -0.2373976707458496\n",
      "\t\t\tLoss: -0.2375727891921997\n",
      "\t\t\tLoss: -0.2377474457025528\n",
      "\t\t\tLoss: -0.23792168498039246\n",
      "\t\t\tLoss: -0.23809555172920227\n",
      "\t\t\tLoss: -0.2382740080356598\n",
      "\t\t\tLoss: -0.23844954371452332\n",
      "\t\t\tLoss: -0.2386246621608734\n",
      "\t\t\tLoss: -0.2387993335723877\n",
      "\t\t\tLoss: -0.23897358775138855\n",
      "\t\t\tLoss: -0.23914743959903717\n",
      "\t\t\tLoss: -0.23932336270809174\n",
      "\t\t\tLoss: -0.23949888348579407\n",
      "\t\t\tLoss: -0.2396739423274994\n",
      "\t\t\tLoss: -0.23984861373901367\n",
      "\t\t\tLoss: -0.24002037942409515\n",
      "\t\t\tLoss: -0.24019169807434082\n",
      "\t\t\tLoss: -0.24036511778831482\n",
      "\t\t\tLoss: -0.24053815007209778\n",
      "\t\t\tLoss: -0.24071070551872253\n",
      "\t\t\tLoss: -0.24088537693023682\n",
      "\t\t\tLoss: -0.24105963110923767\n",
      "\t\t\tLoss: -0.24123342335224152\n",
      "\t\t\tLoss: -0.24140435457229614\n",
      "\t\t\tLoss: -0.24157488346099854\n",
      "\t\t\tLoss: -0.2417449653148651\n",
      "\t\t\tLoss: -0.24191468954086304\n",
      "\t\t\tLoss: -0.24208642542362213\n",
      "\t\t\tLoss: -0.24225780367851257\n",
      "\t\t\tLoss: -0.24242624640464783\n",
      "\t\t\tLoss: -0.24259188771247864\n",
      "\t\t\tLoss: -0.2427571415901184\n",
      "\t\t\tLoss: -0.24292200803756714\n",
      "\t\t\tLoss: -0.24308647215366364\n",
      "\t\t\tLoss: -0.2432505190372467\n",
      "\t\t\tLoss: -0.24341420829296112\n",
      "\t\t\tLoss: -0.2435774803161621\n",
      "\t\t\tLoss: -0.24374037981033325\n",
      "\t\t\tLoss: -0.24390289187431335\n",
      "\t\t\tLoss: -0.24406501650810242\n",
      "\t\t\tLoss: -0.24422675371170044\n",
      "\t\t\tLoss: -0.24438568949699402\n",
      "\t\t\tLoss: -0.24454182386398315\n",
      "\t\t\tLoss: -0.24469763040542603\n",
      "\t\t\tLoss: -0.24485301971435547\n",
      "\t\t\tLoss: -0.24500808119773865\n",
      "\t\t\tLoss: -0.2451627552509308\n",
      "\t\t\tLoss: -0.24531710147857666\n",
      "\t\t\tLoss: -0.2454710602760315\n",
      "\t\t\tLoss: -0.24562229216098785\n",
      "\t\t\tLoss: -0.245775505900383\n",
      "\t\t\tLoss: -0.24592837691307068\n",
      "\t\t\tLoss: -0.2460809051990509\n",
      "\t\t\tLoss: -0.24623307585716248\n",
      "\t\t\tLoss: -0.2463849037885666\n",
      "\t\t\tLoss: -0.24653637409210205\n",
      "\t\t\tLoss: -0.24668510258197784\n",
      "\t\t\tLoss: -0.24683347344398499\n",
      "\t\t\tLoss: -0.24697913229465485\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# Adjust parameters\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# Training on the past iterations\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_train_cycles):\n\u001b[1;32m---> 65\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(loss)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# model.save('rap_music_model.h5')\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:540\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, return_dict)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata\u001b[39m():\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (x, y, sample_weight)\n\u001b[1;32m--> 540\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    541\u001b[0m logs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mmap_structure(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x), logs)\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\lukas\\virtualenvs\\esienv2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "gen = generator(fwd, **sim_params)\n",
    "\n",
    "epochs = 50\n",
    "samples_per_epoch = 64\n",
    "n_train_cycles = 300\n",
    "\n",
    "# Training loop within the RAP-MUSIC framework\n",
    "for epoch in range(epochs):  # Number of epochs\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    # epoch_distances = np.zeros(samples_per_epoch)\n",
    "    for ii in range(samples_per_epoch):\n",
    "        print(f\"\\tsample {ii+1}/{samples_per_epoch}\")\n",
    "        current_data, true_dipoles, Y = generate_initial_data(gen)\n",
    "        n_samples = len(true_dipoles)\n",
    "        n_candidates = len(true_dipoles[0])\n",
    "        estimated_dipole_idc = [list() for _ in range(n_samples)]\n",
    "        \n",
    "        for n_candidate in range(n_candidates):\n",
    "            # print(f\"\\t\\tDipole {n_candidate+1}/{n_candidates}\")\n",
    "            # Compute Covariances\n",
    "            current_covs = np.stack([x@x.T for x in current_data], axis=0)\n",
    "            current_covs = np.stack([cov/abs(cov).max() for cov in current_covs], axis=0)\n",
    "            \n",
    "            # Predict the sources using the model\n",
    "            estimated_sources = model.predict(current_covs, verbose=0)  # Model's prediction\n",
    "            X_train.append(current_covs)\n",
    "\n",
    "            # Check stopping criterion\n",
    "            # criterion = estimated_sources.max(axis=1) > 0.5  # Threshold for stopping (arbitrary value\n",
    "            # if criterion:\n",
    "            #     break\n",
    "            estimated_sources_temp = estimated_sources.copy()\n",
    "            for i_sample in range(n_samples):\n",
    "                estimated_sources_temp[i_sample, estimated_dipole_idc[i_sample]] = 0\n",
    "\n",
    "            new_dipole_idc = np.argmax(estimated_sources_temp, axis=1)  # Convert to dipole indices\n",
    "            \n",
    "            for i_idx, new_idx in enumerate(new_dipole_idc):\n",
    "                estimated_dipole_idc[i_idx].append(new_idx)\n",
    "\n",
    "            true_data_matched = np.zeros((n_samples, n_dipoles))\n",
    "            avg_dists = []\n",
    "            for i_sample in range(n_samples):\n",
    "                true_data_matched[i_sample, true_dipoles[i_sample]] = 1\n",
    "                # estimated_positions = pos[np.array(estimated_dipole_idc[i_sample])]\n",
    "                # true_positions = pos[true_dipoles[i_sample]]\n",
    "                # pairwise_dist = cdist(true_positions, estimated_positions)\n",
    "                # # select the true positions closest to the estimated ones\n",
    "                # true_indices, estimated_indices = linear_sum_assignment(pairwise_dist)\n",
    "                # avg_dists.append(pairwise_dist[true_indices, estimated_indices].min(axis=-1).mean())\n",
    "            # print(\"average distances: \", round(np.mean(avg_dists), 2))\n",
    "            # epoch_distances[epoch] = np.mean(avg_dists)\n",
    "            Y_train.append(true_data_matched)\n",
    "            \n",
    "            # Outproject the dipoles from the respective data\n",
    "            current_data = wrap_outproject_from_data(current_data, leadfield, estimated_dipole_idc)\n",
    "\n",
    "    # Adjust parameters\n",
    "    # Training on the past iterations\n",
    "    for _ in range(n_train_cycles):\n",
    "        loss = model.train_on_batch(np.concatenate(X_train, axis=0), np.concatenate(Y_train, axis=0))\n",
    "        print(f\"\\t\\t\\tLoss: {np.mean(loss)}\")\n",
    "            \n",
    "\n",
    "# Save the model\n",
    "# model.save('rap_music_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gen \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m(fwd, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msim_params)\n\u001b[0;32m      2\u001b[0m X, true_indices, Y \u001b[38;5;241m=\u001b[39m generate_initial_data(gen)\n\u001b[0;32m      3\u001b[0m current_data \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generator' is not defined"
     ]
    }
   ],
   "source": [
    "gen = generator(fwd, **sim_params)\n",
    "X, true_indices, Y = generate_initial_data(gen)\n",
    "current_data = X[0]\n",
    "new_data = outproject_from_data(current_data, leadfield, np.array(true_indices[0])[:1])\n",
    "\n",
    "mne.EvokedArray(current_data, info).plot_topomap()\n",
    "mne.EvokedArray(new_data, info).plot_topomap()\n",
    "\n",
    "new_data = outproject_from_data(current_data, leadfield, np.array(true_indices[0])[1:])\n",
    "mne.EvokedArray(new_data, info).plot_topomap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step\n",
      "No projector specified for this dataset. Please consider the method self.add_proj.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "No projector specified for this dataset. Please consider the method self.add_proj.\n",
      "MLE: 37.05 mm\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "from copy import deepcopy\n",
    "sim_params_temp = deepcopy(sim_params)\n",
    "sim_params_temp[\"batch_size\"] = 1\n",
    "sim_params_temp[\"n_sources\"] = 2\n",
    "sim_params_temp[\"inter_source_correlation\"] = 0.0\n",
    "sim_params_temp[\"correlation_mode\"] = None\n",
    "sim_params_temp[\"snr_range\"] = (1e22, 1e22)\n",
    "sim_params_temp[\"amplitude_range\"] = (1, 1)\n",
    "\n",
    "idx = 0\n",
    "\n",
    "gen = generator(fwd, **sim_params_temp)\n",
    "X, true_indices, Y = generate_initial_data(gen)\n",
    "current_data = deepcopy(X)\n",
    "# Compute Covariances\n",
    "covs = np.stack([x@x.T for x in current_data], axis=0)\n",
    "covs = np.stack([cov/abs(cov).max() for cov in covs], axis=0)\n",
    "estimated_idc = [np.array([]) for _ in range(len(current_data))]\n",
    "\n",
    "for i_iter in range(sim_params_temp[\"n_sources\"]):\n",
    "    predictions = model.predict(covs)\n",
    "    for i_sample in range(len(current_data)):\n",
    "        estimated_idc[i_sample] = np.append(estimated_idc[i_sample], best_match_idx(pos, predictions[i_sample]*max_dist)).astype(int)\n",
    "    \n",
    "    # Create Source Estimate\n",
    "    L = leadfield[:, estimated_idc[idx]]\n",
    "    gradients = np.zeros((n_dipoles, len(estimated_idc[idx])))\n",
    "    for ii, estimated_idx in enumerate(estimated_idc[idx]):\n",
    "        gradients[estimated_idx, ii] = 1\n",
    "    Y_est = gradients @ L.T @ np.linalg.pinv(L @ L.T)\n",
    "    \n",
    "    stc_ = mne.SourceEstimate(Y_est, vertices, tmin=0, tstep=1/1000, \n",
    "                            subject=\"fsaverage\", verbose=0)\n",
    "    \n",
    "    mne.EvokedArray(current_data[idx], info).plot_joint()\n",
    "    \n",
    "    brain = stc_.plot(brain_kwargs=dict(title=f\"Est. Source {i_iter+1}\"), **pp)\n",
    "    brain.add_text(0.1, 0.9, f\"Est. Source {i_iter+1}\", 'title',\n",
    "               font_size=14)\n",
    "\n",
    "    current_data = wrap_outproject_from_data(X, leadfield, estimated_idc)\n",
    "    covs = np.stack([x@x.T for x in current_data], axis=0)\n",
    "    covs = np.stack([cov/abs(cov).max() for cov in covs], axis=0)\n",
    "\n",
    "estimated_positions = pos[estimated_idc[idx]]\n",
    "true_positions = pos[true_indices[idx]]\n",
    "pairwise_dist = cdist(true_positions, estimated_positions)\n",
    "# select the true positions closest to the estimated ones\n",
    "true_sub_idc, estimated_sub_idc = linear_sum_assignment(pairwise_dist)\n",
    "mle = pairwise_dist[true_sub_idc, estimated_sub_idc].mean()\n",
    "print(f\"MLE: {mle:.2f} mm\")\n",
    "\n",
    "L = leadfield[:, estimated_idc[idx]]\n",
    "gradients = np.zeros((n_dipoles, len(estimated_idc[idx])))\n",
    "for ii, estimated_idx in enumerate(estimated_idc[idx]):\n",
    "    gradients[estimated_idx, ii] = 1\n",
    "Y_est = gradients @ L.T @ np.linalg.pinv(L @ L.T)\n",
    "stc_.data = Y_est\n",
    "brain = stc_.plot(brain_kwargs=dict(title=\"Final Source Estimate\"), **pp)\n",
    "brain.add_text(0.1, 0.9, \"Final Source Estimate\", 'title',\n",
    "               font_size=14)\n",
    "\n",
    "\n",
    "stc_.data = Y[idx]\n",
    "brain = stc_.plot(brain_kwargs=dict(title=\"Ground Truth\"), **pp)\n",
    "brain.add_text(0.1, 0.9, \"Ground Truth\", 'title',\n",
    "               font_size=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([189.])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_idc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternating Projections, mle = 0.00 mm\n"
     ]
    }
   ],
   "source": [
    "from invert import Solver\n",
    "n_sources = sim_params_temp[\"n_sources\"]\n",
    "evoked = mne.EvokedArray(X[idx], info)\n",
    "solver = Solver(\"ap\")\n",
    "solver.make_inverse_operator(fwd, evoked, n_orders=0, refine_solution=False, n=n_sources, \n",
    "                             k=n_sources, diffusion_parameter=0.1, stop_crit=0, max_iter=10)\n",
    "\n",
    "stc_ = solver.apply_inverse_operator(evoked)\n",
    "# stc_.data /= abs(stc_.data).max()\n",
    "# brain = stc_.plot(**pp)\n",
    "# brain.add_text(0.1, 0.9, solver.name, 'title',\n",
    "#                font_size=14)\n",
    "\n",
    "# evoked_ = mne.EvokedArray(fwd[\"sol\"][\"data\"] @ stc_.data, info).set_eeg_reference(\"average\", projection=True)\n",
    "# evoked_.plot_joint()\n",
    "\n",
    "# print(solver.name, \" r = \", pearsonr(abs(stc.data).mean(axis=-1), abs(stc_.data).mean(axis=-1))[0])\n",
    "\n",
    "mle = eval_mean_localization_error(Y[idx], stc_.data, adjacency.toarray(), adjacency.toarray(), distance_matrix, mode=\"match\")\n",
    "print(f\"{solver.name}, mle = {mle:.2f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esienv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

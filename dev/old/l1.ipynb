{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.utils import logger, warn\n",
    "from numpy.core.fromnumeric import mean\n",
    "from numpy.lib import diag\n",
    "from scipy.sparse import spdiags\n",
    "\n",
    "from scipy import linalg\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "def groups_norm2(A, n_orient):\n",
    "    \"\"\"Compute squared L2 norms of groups inplace.\"\"\"\n",
    "    n_positions = A.shape[0] // n_orient\n",
    "    return np.sum(np.power(A, 2, A).reshape(n_positions, -1), axis=1)\n",
    "\n",
    "\n",
    "def _solve_lasso(Lw, y, alpha, max_iter):\n",
    "    if y.ndim == 1:\n",
    "        model = linear_model.LassoLars(\n",
    "            max_iter=max_iter, normalize=False, fit_intercept=False, alpha=alpha\n",
    "        )\n",
    "        x = model.fit(Lw, y).coef_.copy()\n",
    "        x = x.T\n",
    "    else:\n",
    "        model = linear_model.MultiTaskLasso(\n",
    "            max_iter=max_iter, normalize=False, fit_intercept=False, alpha=alpha\n",
    "        )\n",
    "        x = model.fit(Lw, y).coef_.copy()\n",
    "        x = x.T\n",
    "    return x\n",
    "\n",
    "\n",
    "def _solve_reweighted_lasso(\n",
    "    L, y, alpha, weights, max_iter, max_iter_reweighting, gprime\n",
    "):\n",
    "    assert max_iter_reweighting > 0\n",
    "\n",
    "    for _ in range(max_iter_reweighting):\n",
    "        L_w = L * weights[np.newaxis, :]\n",
    "        coef_ = _solve_lasso(L_w, y, alpha, max_iter=max_iter)\n",
    "        if y.ndim == 1:\n",
    "            x = coef_ * weights\n",
    "        else:\n",
    "            x = coef_ * weights[:, np.newaxis]\n",
    "        weights = gprime(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def iterative_L1(L, y, alpha=0.2, max_iter=1000, max_iter_reweighting=10):\n",
    "    \"\"\"Iterative Type-I estimator with L1 regularizer.\n",
    "    The optimization objective for iterative estimators in general is::\n",
    "        x^(k+1) <-- argmin_x ||y - Lx||^2_Fro + alpha * sum_i g(x_i)\n",
    "    Which in the case of iterative L1, it boils down to::\n",
    "        x^(k+1) <-- argmin_x ||y - Lx||^2_Fro + alpha * sum_i w_i^(k)|x_i|\n",
    "    Iterative L1::\n",
    "        g(x_i) = log(|x_i| + epsilon)\n",
    "        w_i^(k+1) <-- [|x_i^(k)|+epsilon]\n",
    "    Parameters\n",
    "    ----------\n",
    "    L : array, shape (n_sensors, n_sources)\n",
    "        lead field matrix modeling the forward operator or dictionary matrix\n",
    "    y : array, shape (n_sensors,) or (n_sensors, n_times)\n",
    "        measurement vector, capturing sensor measurements\n",
    "    alpha : float\n",
    "        Constant that makes a trade-off between the data fidelity and regularizer.\n",
    "        Defaults to 1.0\n",
    "    max_iter : int, optional\n",
    "        The maximum number of inner loop iterations\n",
    "    max_iter_reweighting : int, optional\n",
    "        Maximum number of reweighting steps i.e outer loop iterations\n",
    "    Returns\n",
    "    -------\n",
    "    y : array, shape (n_sensors,) or (n_sensors, n_times)\n",
    "        Parameter vector, e.g., source vector in the context of BSI (x in the cost\n",
    "        function formula).\n",
    "    References\n",
    "    ----------\n",
    "    XXX\n",
    "    \"\"\"\n",
    "    eps = np.finfo(float).eps\n",
    "    _, n_sources = L.shape\n",
    "    weights = np.ones(n_sources)\n",
    "    n_orient = 1\n",
    "\n",
    "    def g(w):\n",
    "        return np.sqrt(groups_norm2(w.copy(), n_orient))\n",
    "\n",
    "    def gprime(w):\n",
    "        return np.repeat(g(w), n_orient).ravel() + eps\n",
    "\n",
    "    alpha_max = abs(L.T.dot(y)).max() / len(L)\n",
    "    alpha = alpha * alpha_max\n",
    "\n",
    "    x = _solve_reweighted_lasso(\n",
    "        L, y, alpha, weights, max_iter, max_iter_reweighting, gprime\n",
    "    )\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def iterative_L2(L, y, alpha=0.2, max_iter=1000, max_iter_reweighting=10):\n",
    "    \"\"\"Iterative Type-I estimator with L2 regularizer.\n",
    "    The optimization objective for iterative estimators in general is::\n",
    "        x^(k+1) <-- argmin_x ||y - Lx||^2_Fro + alpha * sum_i g(x_i)\n",
    "    Which in the case of iterative L2, g(x_i) and w_i are defined as follows::\n",
    "    Iterative L2::\n",
    "        g(x_i) = log(x_i^2 + epsilon)\n",
    "        w_i^(k+1) <-- [(x_i^(k))^2+epsilon]\n",
    "    for solving the following problem:\n",
    "        x^(k+1) <-- argmin_x ||y - Lx||^2_Fro + alpha * sum_i w_i^(k)|x_i|\n",
    "    Parameters\n",
    "    ----------\n",
    "    L : array, shape (n_sensors, n_sources)\n",
    "        lead field matrix modeling the forward operator or dictionary matrix\n",
    "    y : array, shape (n_sensors,) or (n_sensors, n_times)\n",
    "        measurement vector, capturing sensor measurements\n",
    "    alpha : float\n",
    "        Constant that makes a trade-off between the data fidelity and regularizer.\n",
    "        Defaults to 0.2.\n",
    "    max_iter : int, optional\n",
    "        The maximum number of inner loop iterations\n",
    "    max_iter_reweighting : int, optional\n",
    "        Maximum number of reweighting steps i.e outer loop iterations\n",
    "    tol : float, optional\n",
    "        The tolerance for the optimization: if the updates are\n",
    "        smaller than ``tol``, the optimization code checks the\n",
    "        dual gap for optimality and continues until it is smaller\n",
    "        than ``tol``.\n",
    "    Returns\n",
    "    -------\n",
    "    y : array, shape (n_sensors,) or (n_sensors, n_times)\n",
    "        Parameter vector, e.g., source vector in the context of BSI (x in the cost\n",
    "        function formula).\n",
    "    References\n",
    "    ----------\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    # XXX : cov is not used\n",
    "    eps = np.finfo(float).eps\n",
    "    _, n_sources = L.shape\n",
    "    weights = np.ones(n_sources)\n",
    "    n_orient = 1\n",
    "\n",
    "    def g(w):\n",
    "        return groups_norm2(w.copy(), n_orient)\n",
    "\n",
    "    def gprime(w):\n",
    "        return np.repeat(g(w), n_orient).ravel() + eps\n",
    "\n",
    "    alpha_max = abs(L.T.dot(y)).max() / len(L)\n",
    "    alpha = alpha * alpha_max\n",
    "\n",
    "    x = _solve_reweighted_lasso(\n",
    "        L, y, alpha, weights, max_iter, max_iter_reweighting, gprime\n",
    "    )\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def iterative_sqrt(L, y, alpha=0.2, max_iter=1000, max_iter_reweighting=10):\n",
    "    \"\"\"Iterative Type-I estimator with L_0.5 regularizer.\n",
    "    The optimization objective for iterative estimators in general is::\n",
    "        x^(k+1) <-- argmin_x ||y - Lx||^2_Fro + alpha * sum_i g(x_i)\n",
    "    Which in the case of iterative \"sqrt\", g(x_i) and w_i are define as follows::\n",
    "    Iterative sqrt (L_0.5)::\n",
    "        g(x_i) = sqrt(|x_i|)\n",
    "        w_i^(k+1) <-- [2sqrt(|x_i|)+epsilon]^-1\n",
    "    for solving the following problem:\n",
    "        x^(k+1) <-- argmin_x ||y - Lx||^2_Fro + alpha * sum_i w_i^(k)|x_i|\n",
    "    Parameters\n",
    "    ----------\n",
    "    L : array, shape (n_sensors, n_sources)\n",
    "        lead field matrix modeling the forward operator or dictionary matrix\n",
    "    y : array, shape (n_sensors,) or (n_sensors, n_times)\n",
    "        measurement vector, capturing sensor measurements\n",
    "    alpha : float\n",
    "        Constant that makes a trade-off between the data fidelity and regularizer.\n",
    "        Defaults to 0.2.\n",
    "    max_iter : int, optional\n",
    "        The maximum number of inner loop iterations\n",
    "    max_iter_reweighting : int, optional\n",
    "        Maximum number of reweighting steps i.e outer loop iterations\n",
    "    Returns\n",
    "    -------\n",
    "    y : array, shape (n_sensors,) or (n_sensors, n_times)\n",
    "        Parameter vector, e.g., source vector in the context of BSI (x in the cost function formula).\n",
    "    References\n",
    "    ----------\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    _, n_sources = L.shape\n",
    "    weights = np.ones(n_sources)\n",
    "    n_orient = 1\n",
    "\n",
    "    def g(w):\n",
    "        return np.sqrt(np.sqrt(groups_norm2(w.copy(), n_orient)))\n",
    "\n",
    "    def gprime(w):\n",
    "        return 2.0 * np.repeat(g(w), n_orient).ravel()\n",
    "\n",
    "    alpha_max = abs(L.T.dot(y)).max() / len(L)\n",
    "    alpha = alpha * alpha_max\n",
    "\n",
    "    x = _solve_reweighted_lasso(\n",
    "        L, y, alpha, weights, max_iter, max_iter_reweighting, gprime\n",
    "    )\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def iterative_L1_typeII(L, y, cov, alpha=0.2, max_iter=1000, max_iter_reweighting=10):\n",
    "    \"\"\"Iterative Type-II estimator with L_1 regularizer.\n",
    "    The optimization objective for iterative Type-II methods is::\n",
    "        x^(k+1) <-- argmin_x ||y - Lx||^2_Fro + alpha * g_SBl(x)\n",
    "    Which in the case of iterative L1 Type-II , g_SBl(x) and w_i are define\n",
    "    as follows::\n",
    "    Iterative-L1-TypeII::\n",
    "        g_SBl(x) = min_{gamma >=0} x^T*Gamma^-1*x + log|alpha*Id + L*Gamma*L^T|\n",
    "        w_i^(k+1) <-- [L_i^T*(lambda*Id + L*hat{W}*hat{X}*L^T)^(-1)*L_i]^(1/2)\n",
    "    where\n",
    "        Gamma = diag(gamma) : souce covariance matrix\n",
    "        hat{W} = diag(W)^-1\n",
    "        hat{X} = diag(X)^-1\n",
    "    for solving the following problem:\n",
    "        x^(k+1) <-- argmin_x ||y - Lx||^2_Fro + alpha * sum_i w_i^(k)|x_i|\n",
    "    NOTE: Please note that lambda models the noise variance and it is a\n",
    "    different paramter than regularization paramter alpha. For simplicity,\n",
    "    we assume lambda = alpha to be consistant with sklearn built-in\n",
    "    function: \"linear_model.LassoLars\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    L : array, shape (n_sensors, n_sources)\n",
    "        lead field matrix modeling the forward operator or dictionary matrix\n",
    "    y : array, shape (n_sensors,) or (n_sensors, n_times)\n",
    "        measurement vector, capturing sensor measurements\n",
    "    cov : array, shape (n_sensors, n_sensors)\n",
    "        noise covariance matrix. If float it corresponds to the noise variance\n",
    "        assumed to be diagonal.\n",
    "    alpha : float\n",
    "        Constant that makes a trade-off between the data fidelity and regularizer.\n",
    "        Defaults to 0.2\n",
    "    max_iter : int, optional\n",
    "        The maximum number of inner loop iterations\n",
    "    max_iter_reweighting : int, optional\n",
    "        Maximum number of reweighting steps i.e outer loop iterations\n",
    "    Returns\n",
    "    -------\n",
    "    y : array, shape (n_sensors,) or (n_sensors, n_times)\n",
    "        Parameter vector, e.g., source vector in the context of BSI (x in the cost\n",
    "        function formula).\n",
    "    References\n",
    "    ----------\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    n_sensors, n_sources = L.shape\n",
    "    weights = np.ones(n_sources)\n",
    "\n",
    "    alpha_max = abs(L.T.dot(y)).max() / len(L)\n",
    "    alpha = alpha * alpha_max\n",
    "\n",
    "    if isinstance(cov, float):\n",
    "        cov = cov * np.eye(n_sensors)\n",
    "\n",
    "    def gprime(coef):\n",
    "        n_orient = 1\n",
    "        L_T = L.T\n",
    "\n",
    "        def g(weights):\n",
    "            return np.sqrt(groups_norm2(weights.copy(), n_orient))\n",
    "\n",
    "        def w_mat(weights):\n",
    "            return np.diag(1.0 / np.repeat(g(weights), n_orient).ravel())\n",
    "\n",
    "        if coef.ndim < 2:\n",
    "            x_mat = np.abs(np.diag(coef))\n",
    "            # X = coef[:, np.newaxis] @ coef[:, np.newaxis].T\n",
    "            # x_mat = np.diag(np.sqrt(np.diag(X)))\n",
    "        else:\n",
    "            X = coef @ coef.T\n",
    "            x_mat = np.diag(linalg.norm(X, axis=0))\n",
    "        noise_cov = cov\n",
    "        proj_source_cov = (L @ np.dot(w_mat(weights), x_mat)) @ L_T\n",
    "        signal_cov = noise_cov + proj_source_cov\n",
    "        sigmaY_inv = linalg.inv(signal_cov)\n",
    "\n",
    "        return 1.0 / (np.sqrt(np.diag((L_T @ sigmaY_inv) @ L)))\n",
    "\n",
    "    x = _solve_reweighted_lasso(\n",
    "        L, y, alpha, weights, max_iter, max_iter_reweighting, gprime\n",
    "    )\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def iterative_L2_typeII(\n",
    "    L, y, cov=1.0, alpha=0.2, max_iter=1000, max_iter_reweighting=10\n",
    "):\n",
    "    \"\"\"Iterative Type-II estimator with L_2 regularizer.\n",
    "    The optimization objective for iterative Type-II methods is::\n",
    "        x^(k+1) <-- argmin_x ||y - Lx||^2_Fro + alpha * g_SBl(x)\n",
    "    Which in the case of iterative L2 Type-II , g_SBl(x) and w_i are define\n",
    "    as follows::\n",
    "    Iterative-L2-TypeII::\n",
    "        g_SBl(x) = min_{gamma >=0} x^T*Gamma^-1*x + log|alpha*Id + L*Gamma*L^T|\n",
    "        w_i^(k+1) <-- [(x_i^(k))^2 + (w_i^(k))^(-1) - (w_i^(k))^(-2) * L_i^T*(lambda*Id + L*hat{W^(k)}*L^T)^(-1)*L_i]^(-1)\n",
    "    where\n",
    "        Gamma = diag(gamma) : souce covariance matrix\n",
    "        hat{W} = diag(W)^-1\n",
    "    for solving the following problem:\n",
    "        x^(k+1) <-- argmin_x ||y - Lx||^2_Fro + alpha * sum_i w_i^(k)|x_i|\n",
    "    Notes\n",
    "    -----\n",
    "    Please note that lambda models the noise variance and it is a\n",
    "    different paramter than regularization paramter alpha. For simplicity,\n",
    "    we assume lambda = alpha to be consistant with sklearn built-in\n",
    "    function: \"linear_model.LassoLars\"\n",
    "    Given the above assumption, one can see the iterative-L2-TypeII\n",
    "    as an extension of its Type-I counterpart where eps is tuned adaptively::\n",
    "    w_i^(k+1) <-- [(x_i^(k))^2+epsilon^(k)]\n",
    "    where\n",
    "    epsilon^(k) = (w_i^(k))^(-1) - (w_i^(k))^(-2) * L_i^T*(lambda*Id + L*hat{W^(k)}*L^T)^(-1)*L_i\n",
    "    Parameters\n",
    "    ----------\n",
    "    L : array, shape (n_sensors, n_sources)\n",
    "        lead field matrix modeling the forward operator or dictionary matrix\n",
    "    y : array, shape (n_sensors,) or (n_sensors, n_times)\n",
    "        measurement vector, capturing sensor measurements\n",
    "    cov : float | array, shape (n_sensors, n_sensors)\n",
    "        noise covariance matrix. If float it corresponds to the noise variance\n",
    "        assumed to be diagonal.\n",
    "    alpha : float\n",
    "        Constant that makes a trade-off between the data fidelity and regularizer.\n",
    "        Defaults to 0.2\n",
    "    max_iter : int, optional\n",
    "        The maximum number of inner loop iterations\n",
    "    max_iter_reweighting : int, optional\n",
    "        Maximum number of reweighting steps i.e outer loop iterations\n",
    "    Returns\n",
    "    -------\n",
    "    x : array, shape (n_sources,) or (n_sources, n_times)\n",
    "        Parameter vector, e.g., source vector in the context of BSI (x in the cost\n",
    "        function formula).\n",
    "    References\n",
    "    ----------\n",
    "    XXX\n",
    "    \"\"\"\n",
    "    n_sensors, n_sources = L.shape\n",
    "    weights = np.ones(n_sources)\n",
    "\n",
    "    alpha_max = abs(L.T.dot(y)).max() / len(L)\n",
    "    alpha = alpha * alpha_max\n",
    "\n",
    "    if isinstance(cov, float):\n",
    "        cov = cov * np.eye(n_sensors)\n",
    "\n",
    "    def gprime(coef):\n",
    "        L_T = L.T\n",
    "        n_samples, _ = L.shape\n",
    "        n_orient = 1\n",
    "\n",
    "        def g(weights):\n",
    "            return np.sqrt(groups_norm2(weights.copy(), n_orient))\n",
    "\n",
    "        def w_mat(weights):\n",
    "            return np.diag(1.0 / np.repeat(g(weights), n_orient).ravel())\n",
    "\n",
    "        def epsilon_update(L, weights, cov):\n",
    "            noise_cov = cov  # extension of method by importing the noise covariance\n",
    "            proj_source_cov = (L @ w_mat(weights)) @ L_T\n",
    "            signal_cov = noise_cov + proj_source_cov\n",
    "            sigmaY_inv = linalg.inv(signal_cov)\n",
    "            return np.diag(\n",
    "                w_mat(weights)\n",
    "                - np.multiply(w_mat(weights ** 2), np.diag((L_T @ sigmaY_inv) @ L))\n",
    "            )\n",
    "\n",
    "        def g_coef(coef):\n",
    "            return groups_norm2(coef.copy(), n_orient)\n",
    "\n",
    "        def gprime_coef(coef):\n",
    "            return np.repeat(g_coef(coef), n_orient).ravel()\n",
    "\n",
    "        return gprime_coef(coef) + epsilon_update(L, weights, cov)\n",
    "\n",
    "    x = _solve_reweighted_lasso(\n",
    "        L, y, alpha, weights, max_iter, max_iter_reweighting, gprime\n",
    "    )\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def gamma_map(\n",
    "    L,\n",
    "    y,\n",
    "    cov=1.0,\n",
    "    alpha=0.2,\n",
    "    max_iter=1000,\n",
    "    tol=1e-15,\n",
    "    update_mode=3,\n",
    "    threshold=1e-5,\n",
    "    gammas=None,\n",
    "    group_size=1,\n",
    "):\n",
    "    \"\"\"Gamma_map method based on MNE package\n",
    "    Parameters\n",
    "    ----------\n",
    "    L : array, shape (n_sensors, n_sources)\n",
    "        lead field matrix modeling the forward operator or dictionary matrix\n",
    "    y : array, shape (n_sensors,)\n",
    "        measurement vector, capturing sensor measurements\n",
    "    cov : float | array, shape (n_sensors, n_sensors)\n",
    "        noise covariance matrix. If float it corresponds to the noise variance\n",
    "        assumed to be diagonal.\n",
    "    alpha : float\n",
    "        Constant that makes a trade-off between the data fidelity and regularizer.\n",
    "        Defaults to 0.2\n",
    "    max_iter : int, optional\n",
    "        The maximum number iterations\n",
    "    tol : float\n",
    "        Tolerance parameter for convergence.\n",
    "    update_mode : int\n",
    "        Update mode, 1: MacKay update, 2: Convex-bounding update (defaul),\n",
    "        3: Expectation-Maximization update\n",
    "    threshold : float\n",
    "        A threshold paramter for forcing to zero the small values in\n",
    "        reconstrcuted gamma in each iteration\n",
    "    gammas : array, shape=(n_sources,)\n",
    "        Initial values for posterior variances (gammas). If None, a\n",
    "        variance of 1.0 is used.\n",
    "    group_size : int\n",
    "        Number of consecutive sources which use the same gamma.\n",
    "    Returns\n",
    "    -------\n",
    "    x : array, shape (n_sources,)\n",
    "        Parameter vector, e.g., source vector in the context of BSI (x in the cost\n",
    "        function formula).\n",
    "    References\n",
    "    ----------\n",
    "    XXX\n",
    "    \"\"\"\n",
    "    eps = np.finfo(float).eps\n",
    "    n_sensors, n_sources = L.shape\n",
    "    if y.ndim < 2:\n",
    "        y = y[:, np.newaxis]\n",
    "    n_times = y.shape[1]\n",
    "    coef = np.zeros((n_sources, n_times))\n",
    "\n",
    "    if isinstance(cov, float):\n",
    "        cov = cov * np.eye(n_sensors)\n",
    "\n",
    "    alpha = mean(np.diag(cov))\n",
    "\n",
    "    if gammas is None:\n",
    "        gammas = np.ones(L.shape[1])\n",
    "        # L_square = np.sum(L ** 2,axis=0)\n",
    "        # inv_L_square = np.zeros(n_sources)\n",
    "        # L_nonzero_index = L_square > 0\n",
    "        # inv_L_square[L_nonzero_index] = 1.0 / L_square[L_nonzero_index]\n",
    "        # w_filter = spdiags(inv_L_square, 0, n_sources, n_sources) @ L.T\n",
    "        # vec_init = mean(mean(w_filter @ y) ** 2)\n",
    "        # gammas = vec_init * np.ones(L.shape[1])\n",
    "\n",
    "    # # # apply normalization so the numerical values are sane\n",
    "    # y_normalize_constant = np.linalg.norm(np.dot(y, y.T), ord='fro')\n",
    "    # y /= np.sqrt(y_normalize_constant)\n",
    "    # alpha /= y_normalize_constant\n",
    "    # cov /= y_normalize_constant\n",
    "    # L_normalize_constant = np.linalg.norm(L, ord=np.inf)\n",
    "    # L /= L_normalize_constant\n",
    "\n",
    "    threshold = 0.2 * alpha\n",
    "\n",
    "    if n_sources % group_size != 0:\n",
    "        raise ValueError(\n",
    "            \"Number of sources has to be evenly dividable by the \" \"group size\"\n",
    "        )\n",
    "\n",
    "    n_active = n_sources\n",
    "    active_set = np.arange(n_sources)\n",
    "\n",
    "    gammas_full_old = gammas.copy()\n",
    "    # x_bar_old = coef\n",
    "\n",
    "    if update_mode == 2:\n",
    "        denom_fun = np.sqrt\n",
    "    elif update_mode == 1:\n",
    "        # do nothing\n",
    "        def denom_fun(x):\n",
    "            return x\n",
    "\n",
    "    elif update_mode == 3:\n",
    "        denom = None\n",
    "    else:\n",
    "        denom = None\n",
    "\n",
    "    last_size = -1\n",
    "    for iter_no in range(max_iter):\n",
    "        gammas[np.isnan(gammas)] = 0.0\n",
    "        gidx = np.abs(gammas) > threshold\n",
    "        active_set = active_set[gidx]\n",
    "        gammas = gammas[gidx]\n",
    "\n",
    "        # update only active gammas (once set to zero it stays at zero)\n",
    "        if n_active > len(active_set):\n",
    "            n_active = active_set.size\n",
    "            L = L[:, gidx]\n",
    "\n",
    "        Sigma_y = np.dot(L * gammas[np.newaxis, :], L.T)\n",
    "        Sigma_y.flat[:: n_sensors + 1] += alpha\n",
    "        # Sigma_y += cov\n",
    "\n",
    "        # Invert CM keeping symmetry\n",
    "        U, S, _ = linalg.svd(Sigma_y, full_matrices=False)\n",
    "        S = S[np.newaxis, :]\n",
    "        del Sigma_y\n",
    "\n",
    "        Sigma_y_inv = np.dot(U / (S + eps), U.T)\n",
    "        Sigma_y_invL = np.dot(Sigma_y_inv, L)\n",
    "        A = np.dot(Sigma_y_invL.T, y)  # mult. w. Diag(gamma) in gamma update\n",
    "\n",
    "        if update_mode == 1:\n",
    "            # MacKay fixed point update\n",
    "            numer = gammas ** 2 * np.mean((A * A.conj()).real, axis=1)\n",
    "            denom = gammas * np.sum(L * Sigma_y_invL, axis=0)\n",
    "        elif update_mode == 2:\n",
    "            # convex-bounding update\n",
    "            numer = gammas * np.sqrt(np.mean((A * A.conj()).real, axis=1))\n",
    "            denom = np.sum(L * Sigma_y_invL, axis=0)  # sqrt is applied below\n",
    "        elif update_mode == 3:\n",
    "            # Expectation Maximization (EM) update\n",
    "            numer = gammas ** 2 * np.mean((A * A.conj()).real, axis=1) + gammas * (\n",
    "                1 - gammas * np.sum(L * Sigma_y_invL, axis=0)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for update_mode\")\n",
    "\n",
    "        if group_size == 1:\n",
    "            if denom is None:\n",
    "                gammas = numer\n",
    "            else:\n",
    "                gammas = numer / np.maximum(denom_fun(denom), np.finfo(\"float\").eps)\n",
    "        else:\n",
    "            numer_comb = np.sum(numer.reshape(-1, group_size), axis=1)\n",
    "            if denom is None:\n",
    "                gammas_comb = numer_comb\n",
    "            else:\n",
    "                denom_comb = np.sum(denom.reshape(-1, group_size), axis=1)\n",
    "                gammas_comb = numer_comb / denom_fun(denom_comb)\n",
    "\n",
    "            gammas = np.repeat(gammas_comb / group_size, group_size)\n",
    "\n",
    "        # compute convergence criterion\n",
    "        gammas_full = np.zeros(n_sources, dtype=np.float64)\n",
    "        gammas_full[active_set] = gammas\n",
    "\n",
    "        # compute the noise covariance\n",
    "        err = np.sum(np.abs(gammas_full - gammas_full_old)) / np.sum(\n",
    "            np.abs(gammas_full_old)\n",
    "        )\n",
    "\n",
    "        # err_x = linalg.norm(x_bar - x_bar_old, ord = 'fro')\n",
    "        # print(err_x)\n",
    "\n",
    "        gammas_full_old = gammas_full\n",
    "\n",
    "        breaking = err < tol or n_active == 0\n",
    "        if len(gammas) != last_size or breaking:\n",
    "            logger.info(\n",
    "                \"Iteration: %d\\t active set size: %d\\t convergence: \"\n",
    "                \"%0.3e\" % (iter_no, len(gammas), err)\n",
    "            )\n",
    "            last_size = len(gammas)\n",
    "\n",
    "        if breaking:\n",
    "            break\n",
    "\n",
    "    if iter_no < max_iter - 1:\n",
    "        logger.info(\"\\nConvergence reached !\\n\")\n",
    "    else:\n",
    "        warn(\"\\nConvergence NOT reached !\\n\")\n",
    "\n",
    "    # undo normalization and compute final posterior mean\n",
    "\n",
    "    # n_const = np.sqrt(y_normalize_constant) / L_normalize_constant\n",
    "    n_const = 1\n",
    "    x_active = n_const * gammas[:, None] * A\n",
    "\n",
    "    coef[active_set, :] = x_active\n",
    "    if n_times == 1:\n",
    "        # x = np.squeeze(coef,axis = 1)\n",
    "        x = coef[:, 0]\n",
    "    else:\n",
    "        x = coef\n",
    "    return x\n",
    "\n",
    "\n",
    "def champagne(L, y, cov=1.0, alpha=0.2, max_iter=1000, max_iter_reweighting=10):\n",
    "    \"\"\"Champagne method based on our MATLAB codes\n",
    "    Parameters\n",
    "    ----------\n",
    "    L : array, shape (n_sensors, n_sources)\n",
    "        lead field matrix modeling the forward operator or dictionary matrix\n",
    "    y : array, shape (n_sensors,)\n",
    "        measurement vector, capturing sensor measurements\n",
    "    cov : float | array, shape (n_sensors, n_sensors)\n",
    "        noise covariance matrix. If float it corresponds to the noise variance\n",
    "        assumed to be diagonal.\n",
    "    alpha : float\n",
    "        Constant that makes a trade-off between the data fidelity and regularizer.\n",
    "        Defaults to 0.2\n",
    "    max_iter : int, optional\n",
    "        The maximum number of inner loop iterations\n",
    "    max_iter_reweighting : int, optional\n",
    "        Maximum number of reweighting steps i.e outer loop iterations\n",
    "    Returns\n",
    "    -------\n",
    "    x : array, shape (n_sources,)\n",
    "        Parameter vector, e.g., source vector in the context of BSI (x in the cost\n",
    "        function formula).\n",
    "    References\n",
    "    ----------\n",
    "    XXX\n",
    "    \"\"\"\n",
    "    n_sensors, n_sources = L.shape\n",
    "    _, n_times = y.shape\n",
    "    gammas = np.ones(n_sources)\n",
    "    eps = np.finfo(float).eps\n",
    "    threshold = 0.2 * mean(diag(cov))\n",
    "    x = np.zeros((n_sources, n_times))\n",
    "    n_active = n_sources\n",
    "    active_set = np.arange(n_sources)\n",
    "    # H = np.concatenate(L, np.eyes(n_sensors), axis = 1)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        gammas[np.isnan(gammas)] = 0.0\n",
    "        gidx = np.abs(gammas) > threshold\n",
    "        active_set = active_set[gidx]\n",
    "        gammas = gammas[gidx]\n",
    "\n",
    "        # update only active gammas (once set to zero it stays at zero)\n",
    "        if n_active > len(active_set):\n",
    "            n_active = active_set.size\n",
    "            L = L[:, gidx]\n",
    "\n",
    "        Gamma = spdiags(gammas, 0, len(active_set), len(active_set))\n",
    "        Sigma_y = (L @ Gamma @ L.T) + cov\n",
    "        U, S, _ = linalg.svd(Sigma_y, full_matrices=False)\n",
    "        S = S[np.newaxis, :]\n",
    "        del Sigma_y\n",
    "        Sigma_y_inv = np.dot(U / (S + eps), U.T)\n",
    "        # Sigma_y_inv = linalg.inv(Sigma_y)\n",
    "        x_bar = Gamma @ L.T @ Sigma_y_inv @ y\n",
    "        gammas = np.sqrt(\n",
    "            np.diag(x_bar @ x_bar.T / n_times) / np.diag(L.T @ Sigma_y_inv @ L)\n",
    "        )\n",
    "        e_bar = y - (L @ x_bar)\n",
    "        cov = np.sqrt(np.diag(e_bar @ e_bar.T / n_times) / np.diag(Sigma_y_inv))\n",
    "        threshold = 0.2 * mean(diag(cov))\n",
    "\n",
    "    x[active_set, :] = x_bar\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt\n",
    "import sys; sys.path.insert(0, '../')\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import mne\n",
    "from esinet import Simulation\n",
    "from esinet.forward import get_info, create_forward_model\n",
    "from esinet.util import unpack_fwd\n",
    "pp = dict(surface='white', hemi='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Forward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    2.5s remaining:    2.5s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.2s remaining:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "info = get_info(kind='biosemi64')\n",
    "fwd = create_forward_model(info=info, sampling='ico3')\n",
    "\n",
    "leadfield, pos = unpack_fwd(fwd)[1:3]\n",
    "n_chans, n_dipoles = leadfield.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- number of adjacent vertices : 1284\n",
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  4.23it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2005.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data shape:  (1284, 25) (1284, 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 401.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [2.82071877e-09 4.45331951e-09 2.11753782e-08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For automatic theme detection, \"darkdetect\" has to be installed! You can install it with `pip install darkdetect`\n",
      "To use light mode, \"qdarkstyle\" has to be installed! You can install it with `pip install qdarkstyle`\n"
     ]
    }
   ],
   "source": [
    "settings = dict(number_of_sources=3, extents=(40,50), duration_of_trial=0.025, target_snr=1e29)\n",
    "\n",
    "sim = Simulation(fwd, info, settings).simulate(2)\n",
    "stc = sim.source_data[0]\n",
    "evoked = sim.eeg_data[0].average()\n",
    "\n",
    "brain = stc.plot(**pp)\n",
    "brain.add_text(0.1, 0.9, 'Ground Truth', 'title',\n",
    "               font_size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.087136463680781e-07\n",
      "Using control points [0.00000000e+00 0.00000000e+00 4.56389119e-08]\n",
      "For automatic theme detection, \"darkdetect\" has to be installed! You can install it with `pip install darkdetect`\n",
      "To use light mode, \"qdarkstyle\" has to be installed! You can install it with `pip install qdarkstyle`\n"
     ]
    }
   ],
   "source": [
    "y = sim.eeg_data[0].average().data\n",
    "cov = np.identity(n_chans)\n",
    "y_hat = champagne(leadfield, y, cov=cov, alpha=0.01)\n",
    "print(np.max(abs(y_hat)))\n",
    "\n",
    "stc_hat = stc.copy()\n",
    "stc_hat.data = y_hat\n",
    "brain = stc_hat.plot(**pp)\n",
    "brain.add_text(0.1, 0.9, 'Iterative L1', 'title',\n",
    "               font_size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('invertenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "109cbb5ed194d0e1c7aea844cf0a4a10faadf2a56a1c0eb03142356ad9dcb9c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt\n",
    "import sys; sys.path.insert(0, '../')\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import mne\n",
    "from esinet import Simulation\n",
    "from esinet.forward import get_info, create_forward_model\n",
    "from esinet.util import unpack_fwd\n",
    "pp = dict(surface='white', hemi='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(sim):\n",
    "    X = np.stack([eeg.average().data for eeg in sim.eeg_data])\n",
    "    y = np.stack([src.data for src in sim.source_data])\n",
    "    for i, (x_sample, y_sample) in enumerate(zip(X, y)):\n",
    "        # X[i] = np.stack([(x - np.mean(x)) / np.std(x) for x in x_sample.T], axis=0).T\n",
    "        # y[i] = np.stack([ y / np.max(abs(y)) for y in y_sample.T], axis=0).T\n",
    "\n",
    "        X[i] = np.stack([x - np.mean(x) for x in x_sample.T], axis=0).T\n",
    "        X[i] /= np.linalg.norm(X[i])\n",
    "        y[i] /= np.max(abs(y[i]))\n",
    "\n",
    "    X = np.swapaxes(X, 1,2)\n",
    "    y = np.swapaxes(y, 1,2)\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "def make_mask(y, thresh=0.001):\n",
    "    y_mask = np.zeros((y.shape[0], y.shape[-1]))\n",
    "    for i, y_samp in enumerate(y):\n",
    "        yy = abs(y_samp).mean(axis=0)\n",
    "        \n",
    "\n",
    "        y_mask[i] = (yy > yy.max()*thresh).astype(int)\n",
    "    return y_mask\n",
    "\n",
    "def get_components(X, leadfield_norm):\n",
    "    X_components = np.stack([leadfield_norm.T @ X_sample.T for X_sample in X], axis=0)\n",
    "    X_components = np.swapaxes(X_components, 1, 2)\n",
    "    return np.abs(X_components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    2.1s remaining:    3.6s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    2.2s remaining:    1.3s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.0s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done   3 out of   8 | elapsed:    0.1s remaining:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done   5 out of   8 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done   8 out of   8 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "info = get_info(kind='biosemi128')\n",
    "fwd = create_forward_model(info=info, sampling='ico3')\n",
    "\n",
    "leadfield, pos = unpack_fwd(fwd)[1:3]\n",
    "leadfield -= leadfield.mean(axis=0)\n",
    "leadfield_norm = leadfield / np.linalg.norm(leadfield, axis=0)\n",
    "\n",
    "n_chans, n_dipoles = leadfield.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- number of adjacent vertices : 1284\n",
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:11<00:00, 251.47it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 25208.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data shape:  (1284, 10) (1284, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:08<00:00, 362.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# settings = dict(number_of_sources=(1, 15), extents=(1, 25), duration_of_trial=0.001, target_snr=1e99)\n",
    "settings = dict(number_of_sources=(1, 25), extents=(1, 2), duration_of_trial=0.01, target_snr=1e99, source_number_weighting=False)\n",
    "\n",
    "\n",
    "sim = Simulation(fwd, info, settings).simulate(10000)\n",
    "stc = sim.source_data[0]\n",
    "evoked = sim.eeg_data[0].average()\n",
    "\n",
    "# stc.data /= abs(stc.data).max()\n",
    "# brain = stc.plot(**pp)\n",
    "# brain.add_text(0.1, 0.9, 'Ground Truth', 'title',\n",
    "#                font_size=14)\n",
    "# evoked.plot_joint()\n",
    "\n",
    "X, y = prep_data(sim)\n",
    "y_mask = make_mask(y, thresh=0.001)\n",
    "X_components = get_components(X, leadfield_norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Prelocalizer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, None, 1284)]      0         \n",
      "                                                                 \n",
      " time_distributed_34 (TimeDi  (None, None, 300)        385500    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_35 (TimeDi  (None, None, 300)        90300     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_36 (TimeDi  (None, None, 300)        90300     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_37 (TimeDi  (None, None, 1284)       386484    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 952,584\n",
      "Trainable params: 952,584\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "80/80 [==============================] - 3s 30ms/step - loss: 0.0383 - val_loss: 0.0379\n",
      "Epoch 2/50\n",
      "80/80 [==============================] - 2s 29ms/step - loss: 0.0371 - val_loss: 0.0369\n",
      "Epoch 3/50\n",
      "80/80 [==============================] - 2s 30ms/step - loss: 0.0360 - val_loss: 0.0367\n",
      "Epoch 4/50\n",
      "80/80 [==============================] - 2s 29ms/step - loss: 0.0353 - val_loss: 0.0360\n",
      "Epoch 5/50\n",
      "80/80 [==============================] - 2s 27ms/step - loss: 0.0349 - val_loss: 0.0359\n",
      "Epoch 6/50\n",
      "80/80 [==============================] - 2s 26ms/step - loss: 0.0344 - val_loss: 0.0357\n",
      "Epoch 7/50\n",
      "80/80 [==============================] - 2s 26ms/step - loss: 0.0340 - val_loss: 0.0355\n",
      "Epoch 8/50\n",
      "80/80 [==============================] - 2s 25ms/step - loss: 0.0335 - val_loss: 0.0355\n",
      "Epoch 9/50\n",
      "80/80 [==============================] - 2s 26ms/step - loss: 0.0330 - val_loss: 0.0354\n",
      "Epoch 10/50\n",
      "80/80 [==============================] - 2s 26ms/step - loss: 0.0328 - val_loss: 0.0352\n",
      "Epoch 11/50\n",
      "80/80 [==============================] - 2s 27ms/step - loss: 0.0322 - val_loss: 0.0353\n",
      "Epoch 12/50\n",
      "80/80 [==============================] - 2s 27ms/step - loss: 0.0317 - val_loss: 0.0353\n",
      "Epoch 13/50\n",
      "80/80 [==============================] - 2s 26ms/step - loss: 0.0315 - val_loss: 0.0351\n",
      "Epoch 14/50\n",
      "80/80 [==============================] - 2s 27ms/step - loss: 0.0312 - val_loss: 0.0355\n",
      "Epoch 15/50\n",
      "80/80 [==============================] - 2s 27ms/step - loss: 0.0308 - val_loss: 0.0358\n",
      "Epoch 16/50\n",
      "80/80 [==============================] - 2s 28ms/step - loss: 0.0304 - val_loss: 0.0355\n",
      "Epoch 17/50\n",
      "80/80 [==============================] - 2s 29ms/step - loss: 0.0303 - val_loss: 0.0359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b78a887c70>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, multiply, Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "n_time = X_components.shape[1]\n",
    "\n",
    "y_mask_new = deepcopy(y_mask)\n",
    "y_mask_new = y_mask_new[:, np.newaxis]\n",
    "y_mask_new = np.repeat(y_mask_new, n_time, axis=1)\n",
    "\n",
    "n_dense_units = 300\n",
    "n_lstm_units = 64\n",
    "activation_function = \"elu\"\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "dropout = 0.2\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_dipoles), name='Input')\n",
    "\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function,\n",
    "            name='FC1'))(inputs)\n",
    "\n",
    "fc2 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function,\n",
    "            name='FC2'))(fc1)\n",
    "\n",
    "fc3 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function,\n",
    "            name='FC3'))(fc2)\n",
    "\n",
    "\n",
    "# Masking\n",
    "# lstm1 = Bidirectional(LSTM(n_lstm_units, return_sequences=False, \n",
    "#             input_shape=(None, n_dense_units)), \n",
    "#             name='LSTM1')(fc3)\n",
    "# lstm1 = Dense(n_dipoles, \n",
    "#             activation=\"sigmoid\", \n",
    "#             name='Mask')(lstm1)\n",
    "\n",
    "out = TimeDistributed(Dense(n_dipoles, \n",
    "            activation=\"softmax\", \n",
    "            # activity_regularizer=tf.keras.regularizers.L1(l1=0.001),\n",
    "            name='Output'))(fc3)\n",
    "\n",
    "# out = multiply([lstm1, out])\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=out, name='Prelocalizer')\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=4, min_delta=0.00, monitor=\"val_loss\", restore_best_weights=True)]\n",
    "model.fit(X_components, y_mask_new, epochs=epochs, batch_size=batch_size, validation_split=0.15, callbacks=callbacks)\n",
    "# model.fit(X[:, 0], y_mask, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Prelocalizer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (InputLayer)          [(None, None, 1284)]      0         \n",
      "                                                                 \n",
      " time_distributed_40 (TimeDi  (None, None, 300)        385500    \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_41 (TimeDi  (None, None, 300)        90300     \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " LSTM1 (LSTM)                [(None, None, 150),       270600    \n",
      "                              (None, 150),                       \n",
      "                              (None, 150)]                       \n",
      "                                                                 \n",
      " Mask (Dense)                (None, 1284)              193884    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 940,284\n",
      "Trainable params: 940,284\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "80/80 [==============================] - 5s 35ms/step - loss: 0.0807 - val_loss: 0.0351\n",
      "Epoch 2/40\n",
      "80/80 [==============================] - 2s 27ms/step - loss: 0.0341 - val_loss: 0.0340\n",
      "Epoch 3/40\n",
      "80/80 [==============================] - 2s 26ms/step - loss: 0.0339 - val_loss: 0.0341\n",
      "Epoch 4/40\n",
      "80/80 [==============================] - 2s 25ms/step - loss: 0.0339 - val_loss: 0.0339\n",
      "Epoch 5/40\n",
      "80/80 [==============================] - 2s 27ms/step - loss: 0.0338 - val_loss: 0.0340\n",
      "Epoch 6/40\n",
      "80/80 [==============================] - 2s 26ms/step - loss: 0.0336 - val_loss: 0.0339\n",
      "Epoch 7/40\n",
      "80/80 [==============================] - 2s 25ms/step - loss: 0.0336 - val_loss: 0.0337\n",
      "Epoch 8/40\n",
      "80/80 [==============================] - 2s 24ms/step - loss: 0.0334 - val_loss: 0.0338\n",
      "Epoch 9/40\n",
      "80/80 [==============================] - 2s 24ms/step - loss: 0.0334 - val_loss: 0.0339\n",
      "Epoch 10/40\n",
      "80/80 [==============================] - 2s 24ms/step - loss: 0.0333 - val_loss: 0.0336\n",
      "Epoch 11/40\n",
      "80/80 [==============================] - 2s 26ms/step - loss: 0.0332 - val_loss: 0.0338\n",
      "Epoch 12/40\n",
      "80/80 [==============================] - 2s 23ms/step - loss: 0.0332 - val_loss: 0.0336\n",
      "Epoch 13/40\n",
      "80/80 [==============================] - 2s 22ms/step - loss: 0.0329 - val_loss: 0.0335\n",
      "Epoch 14/40\n",
      "80/80 [==============================] - 2s 23ms/step - loss: 0.0328 - val_loss: 0.0335\n",
      "Epoch 15/40\n",
      "80/80 [==============================] - 2s 23ms/step - loss: 0.0327 - val_loss: 0.0334\n",
      "Epoch 16/40\n",
      "80/80 [==============================] - 2s 22ms/step - loss: 0.0325 - val_loss: 0.0332\n",
      "Epoch 17/40\n",
      "80/80 [==============================] - 2s 23ms/step - loss: 0.0321 - val_loss: 0.0328\n",
      "Epoch 18/40\n",
      "80/80 [==============================] - 2s 23ms/step - loss: 0.0316 - val_loss: 0.0325\n",
      "Epoch 19/40\n",
      "80/80 [==============================] - 2s 23ms/step - loss: 0.0309 - val_loss: 0.0318\n",
      "Epoch 20/40\n",
      "80/80 [==============================] - 2s 23ms/step - loss: 0.0303 - val_loss: 0.0313\n",
      "Epoch 21/40\n",
      "80/80 [==============================] - 2s 22ms/step - loss: 0.0295 - val_loss: 0.0307\n",
      "Epoch 22/40\n",
      "80/80 [==============================] - 2s 22ms/step - loss: 0.0290 - val_loss: 0.0304\n",
      "Epoch 23/40\n",
      "80/80 [==============================] - 2s 23ms/step - loss: 0.0282 - val_loss: 0.0299\n",
      "Epoch 24/40\n",
      "80/80 [==============================] - 2s 22ms/step - loss: 0.0276 - val_loss: 0.0304\n",
      "Epoch 25/40\n",
      "80/80 [==============================] - 2s 22ms/step - loss: 0.0268 - val_loss: 0.0296\n",
      "Epoch 26/40\n",
      "80/80 [==============================] - 2s 23ms/step - loss: 0.0262 - val_loss: 0.0299\n",
      "Epoch 27/40\n",
      "80/80 [==============================] - 2s 22ms/step - loss: 0.0257 - val_loss: 0.0294\n",
      "Epoch 28/40\n",
      "80/80 [==============================] - 2s 22ms/step - loss: 0.0249 - val_loss: 0.0293\n",
      "Epoch 29/40\n",
      "80/80 [==============================] - 2s 23ms/step - loss: 0.0242 - val_loss: 0.0295\n",
      "Epoch 30/40\n",
      "80/80 [==============================] - 2s 23ms/step - loss: 0.0236 - val_loss: 0.0293\n",
      "Epoch 31/40\n",
      "80/80 [==============================] - 2s 22ms/step - loss: 0.0229 - val_loss: 0.0292\n",
      "Epoch 32/40\n",
      "80/80 [==============================] - 2s 23ms/step - loss: 0.0222 - val_loss: 0.0292\n",
      "Epoch 33/40\n",
      "80/80 [==============================] - 2s 23ms/step - loss: 0.0217 - val_loss: 0.0287\n",
      "Epoch 34/40\n",
      "80/80 [==============================] - 2s 24ms/step - loss: 0.0209 - val_loss: 0.0289\n",
      "Epoch 35/40\n",
      "80/80 [==============================] - 2s 24ms/step - loss: 0.0203 - val_loss: 0.0291\n",
      "Epoch 36/40\n",
      "80/80 [==============================] - 2s 25ms/step - loss: 0.0195 - val_loss: 0.0294\n",
      "Epoch 37/40\n",
      "80/80 [==============================] - 2s 24ms/step - loss: 0.0187 - val_loss: 0.0291\n",
      "Epoch 38/40\n",
      "80/80 [==============================] - 2s 24ms/step - loss: 0.0182 - val_loss: 0.0293\n",
      "Epoch 39/40\n",
      "80/80 [==============================] - 2s 24ms/step - loss: 0.0175 - val_loss: 0.0298\n",
      "Epoch 40/40\n",
      "80/80 [==============================] - 2s 24ms/step - loss: 0.0168 - val_loss: 0.0295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b6997dcbb0>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed, Bidirectional, LSTM, multiply, Dropout, Activation\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "n_channels, n_dipoles = leadfield.shape\n",
    "input_shape = (None, None, n_channels)\n",
    "tf.keras.backend.set_image_data_format('channels_last')\n",
    "\n",
    "n_dense_units = 300\n",
    "n_lstm_units = 150\n",
    "activation_function = \"elu\"\n",
    "batch_size = 32\n",
    "epochs = 40\n",
    "dropout = 0.2\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None, n_dipoles), name='Input')\n",
    "\n",
    "\n",
    "fc1 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function,\n",
    "            name='FC1'))(inputs)\n",
    "\n",
    "fc2 = TimeDistributed(Dense(n_dense_units, \n",
    "            activation=activation_function,\n",
    "            name='FC2'))(fc1)\n",
    "\n",
    "lstm1 = LSTM(n_lstm_units, return_sequences=True, return_state=True,\n",
    "            name='LSTM1')(fc2)[2]\n",
    "\n",
    "out = Dense(n_dipoles, \n",
    "            activation=\"softmax\", \n",
    "            # activity_regularizer=tf.keras.regularizers.L1(l1=0.0001),\n",
    "            name='Mask')(lstm1)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=out, name='Prelocalizer')\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=10, min_delta=0.00, monitor=\"val_loss\", restore_best_weights=True)]\n",
    "\n",
    "model.fit(X_components[:], y_mask[:], epochs=epochs, batch_size=batch_size, validation_split=0.15, callbacks=callbacks)\n",
    "# model.fit(X[:, 0], y_mask, epochs=epochs, batch_size=batch_size, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- number of adjacent vertices : 1284\n",
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.55it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 2004.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data shape:  (1284, 10) (1284, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 332.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 71ms/step\n",
      "(0.0001492844317592748, 0.9957360526073976)\n"
     ]
    }
   ],
   "source": [
    "# settings = dict(number_of_sources=(1, 15), extents=(1, 25), duration_of_trial=0.001, target_snr=1e99)\n",
    "settings = dict(number_of_sources=3, extents=1, duration_of_trial=0.01, target_snr=1e99)\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings).simulate(2)\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "y_test_mask = make_mask(y_test, thresh=0.001)\n",
    "X_test_components = get_components(X_test, leadfield_norm)\n",
    "y_hat = model.predict(X_test_components)\n",
    "if len(y_hat.shape) == 3:\n",
    "    y_hat = y_hat[:, 0]\n",
    "\n",
    "# y_hat = model.predict(X_test[:, 0])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(y_hat[0] / np.linalg.norm(y_hat[0]))\n",
    "\n",
    "# plt.figure()\n",
    "plt.plot(y_test_mask[0] / np.linalg.norm(y_test_mask[0]))\n",
    "print(pearsonr(y_test_mask[0], y_hat[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- number of adjacent vertices : 1284\n",
      "Simulating data based on sparse patches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  4.26it/s]\n",
      "100%|██████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source data shape:  (1284, 10) (1284, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 286.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.         0.         0.55704243]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For automatic theme detection, \"darkdetect\" has to be installed! You can install it with `pip install darkdetect`\n",
      "To use light mode, \"qdarkstyle\" has to be installed! You can install it with `pip install qdarkstyle`\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Using control points [3.60627244e-13 2.22251818e-03 5.45300780e-01]\n",
      "For automatic theme detection, \"darkdetect\" has to be installed! You can install it with `pip install darkdetect`\n",
      "To use light mode, \"qdarkstyle\" has to be installed! You can install it with `pip install qdarkstyle`\n",
      "No projector specified for this dataset. Please consider the method self.add_proj.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b6d26f8b20>]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using control points [0.       0.       0.685455]\n",
      "Using control points [3.07227231e-13 2.91263785e-03 3.39385474e-01]\n"
     ]
    }
   ],
   "source": [
    "settings = dict(number_of_sources=5, extents=1, duration_of_trial=0.01, target_snr=1e99)\n",
    "\n",
    "sim_test = Simulation(fwd, info, settings).simulate(2)\n",
    "stc = sim_test.source_data[0]\n",
    "evoked = sim_test.eeg_data[0].average()\n",
    "\n",
    "stc.data /= abs(stc.data).max()\n",
    "brain = stc.plot(**pp)\n",
    "brain.add_text(0.1, 0.9, 'Ground Truth', 'title',\n",
    "               font_size=14)\n",
    "evoked.plot_joint()\n",
    "\n",
    "X_test, y_test = prep_data(sim_test)\n",
    "y_mask_test = make_mask(y_test, thresh=0.001)\n",
    "X_test_components = get_components(X_test, leadfield_norm)\n",
    "\n",
    "gammas = model.predict(X_test_components)[0]\n",
    "\n",
    "# gammas[gammas<gammas.max()*0.1] = 0\n",
    "# gammas[gammas<np.percentile(gammas, 90)] = 0\n",
    "\n",
    "from invert.util import find_corner\n",
    "\n",
    "\n",
    "if len(gammas.shape) == 2:\n",
    "    for i in range(len(gammas)):\n",
    "        gammas[i][gammas[i]<gammas[i].max()*0.01] = 0\n",
    "\n",
    "    y_hat = np.stack([ \n",
    "        np.linalg.pinv(np.diag(gamma!=0) @ leadfield.T).T @ X_test[0,0] \n",
    "        for gamma in gammas], axis=1)\n",
    "    \n",
    "    # y_hat = np.stack([ np.diag(gamma) @ leadfield.T @ X_test[0,0] for gamma in gammas], axis=1)\n",
    "    x_hat = leadfield @ y_hat\n",
    "else:\n",
    "    # Thresholding Gammas:\n",
    "    idc = np.argsort(gammas)[::-1]\n",
    "    iters = np.arange(len(gammas))\n",
    "    idx = find_corner(iters, gammas[idc])\n",
    "    thresh = gammas[idc[idx]]\n",
    "    gammas[gammas<thresh] = 0\n",
    "    gidx = gammas!=0\n",
    "    y_hat = np.linalg.pinv(np.diag(gammas!=0).astype(int) @ leadfield.T).T @ X_test[0].T\n",
    "    \n",
    "    x_hat = leadfield @ y_hat\n",
    "\n",
    "stc_ = stc.copy()\n",
    "stc_.data = y_hat / abs(y_hat).max()\n",
    "\n",
    "brain = stc_.plot(**pp)\n",
    "brain.add_text(0.1, 0.9, 'Predicted Mask', 'title',\n",
    "               font_size=14)\n",
    "evoked_ = mne.EvokedArray(x_hat, info)\n",
    "evoked_.plot_joint()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(y_hat[:, 0] / np.linalg.norm(y_hat[:, 0]))\n",
    "\n",
    "# plt.figure()\n",
    "plt.plot(y_mask_test[0] / np.linalg.norm(y_mask_test[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b7630a1100>]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.subplot(311)\n",
    "plt.plot(gammas[0])\n",
    "\n",
    "plt.subplot(312)\n",
    "plt.plot(gammas[1])\n",
    "\n",
    "plt.subplot(313)\n",
    "plt.plot(gammas[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('invertenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dda1e5657e486f74a7b39841fb8103db2af51a77394f44c39a7821a371af47bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
